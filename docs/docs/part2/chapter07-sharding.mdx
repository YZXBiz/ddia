---
sidebar_position: 2
title: "Chapter 7. Sharding"
description: "Understanding data sharding and partitioning strategies for distributed databases"
---

import { Box, Arrow, Row, Column, Group, DiagramContainer, ProcessFlow, CardGrid, ConnectionDiagram, StackDiagram, colors } from '@site/src/components/diagrams/Diagram';

# Chapter 7. Sharding

> Clearly, we must break away from the sequential and not limit the computers. We must state definitions and provide for priorities and descriptions of data. We must state relationships, not procedures.
>
> Grace Murray Hopper, Management and the Computer of the Future (1962)

## Table of Contents

1. [Sharding and Partitioning](#1-sharding-and-partitioning)
2. [Pros and Cons of Sharding](#2-pros-and-cons-of-sharding)
3. [Sharding for Multitenancy](#3-sharding-for-multitenancy)
   - 3.1. [Resource isolation](#31-resource-isolation)
   - 3.2. [Permission isolation](#32-permission-isolation)
   - 3.3. [Cell-based architecture](#33-cell-based-architecture)
   - 3.4. [Per-tenant backup and restore](#34-per-tenant-backup-and-restore)
   - 3.5. [Regulatory compliance](#35-regulatory-compliance)
   - 3.6. [Data residence](#36-data-residence)
   - 3.7. [Gradual schema rollout](#37-gradual-schema-rollout)
4. [Sharding of Key-Value Data](#4-sharding-of-key-value-data)
5. [Sharding by Key Range](#5-sharding-by-key-range)
   - 5.1. [Rebalancing key-range sharded data](#51-rebalancing-key-range-sharded-data)
6. [Sharding by Hash of Key](#6-sharding-by-hash-of-key)
   - 6.1. [Hash modulo number of nodes](#61-hash-modulo-number-of-nodes)
   - 6.2. [Fixed number of shards](#62-fixed-number-of-shards)
   - 6.3. [Sharding by hash range](#63-sharding-by-hash-range)
7. [Partitioning and Range Queries in Data Warehouses](#7-partitioning-and-range-queries-in-data-warehouses)
   - 7.1. [Consistent hashing](#71-consistent-hashing)
8. [Skewed Workloads and Relieving Hot Spots](#8-skewed-workloads-and-relieving-hot-spots)
9. [Operations: Automatic or Manual Rebalancing](#9-operations-automatic-or-manual-rebalancing)
10. [Request Routing](#10-request-routing)
11. [Sharding and Secondary Indexes](#11-sharding-and-secondary-indexes)
   - 11.1. [Local Secondary Indexes](#111-local-secondary-indexes)
   - 11.2. [Global Secondary Indexes](#112-global-secondary-indexes)
12. [Summary](#12-summary)

**In plain English:** Think of sharding like organizing a massive library. When one building can't hold all the books anymore, you create multiple branches across the city. Each branch holds different books (different shards), and you need a system to know which branch has which books.

**In technical terms:** Sharding splits a large dataset into smaller partitions, with each shard stored on a different node. Each piece of data belongs to exactly one shard, though shards are typically replicated for fault tolerance.

**Why it matters:** Sharding enables horizontal scaling‚Äîyou can grow your database by adding more machines instead of needing a single massive server. This makes it possible to handle petabyte-scale datasets and millions of queries per second.

<DiagramContainer title="Combining Replication and Sharding">
  <Column>
    <Row>
      <Group title="Shard 1" color={colors.blue}>
        <Box color={colors.blue} size="sm">Leader</Box>
        <Row gap="sm">
          <Box color={colors.cyan} variant="outlined" size="sm">Follower A</Box>
          <Box color={colors.cyan} variant="outlined" size="sm">Follower B</Box>
        </Row>
      </Group>
      <Group title="Shard 2" color={colors.purple}>
        <Box color={colors.purple} size="sm">Leader</Box>
        <Row gap="sm">
          <Box color={colors.pink} variant="outlined" size="sm">Follower A</Box>
          <Box color={colors.pink} variant="outlined" size="sm">Follower B</Box>
        </Row>
      </Group>
    </Row>
    <Box variant="subtle" color={colors.slate}>
      Each shard has one leader and multiple followers
      <br />
      Each node acts as leader for some shards, follower for others
    </Box>
  </Column>
</DiagramContainer>

A distributed database typically distributes data across nodes in two ways:

Having a copy of the same data on multiple nodes: this is replication, which we discussed in Chapter 6.

If we don't want every node to store all the data, we can split up a large amount of data into smaller shards or partitions, and store different shards on different nodes. We'll discuss sharding in this chapter.

Normally, shards are defined in such a way that each piece of data (each record, row, or document) belongs to exactly one shard. There are various ways of achieving this, which we discuss in depth in this chapter. In effect, each shard is a small database of its own, although some database systems support operations that touch multiple shards at the same time.

Sharding is usually combined with replication so that copies of each shard are stored on multiple nodes. This means that, even though each record belongs to exactly one shard, it may still be stored on several different nodes for fault tolerance.

A node may store more than one shard. If a single-leader replication model is used, the combination of sharding and replication can look like the diagram above. Each shard's leader is assigned to one node, and its followers are assigned to other nodes. Each node may be the leader for some shards and a follower for other shards, but each shard still only has one leader.

Everything we discussed in Chapter 6 about replication of databases applies equally to replication of shards. Since the choice of sharding scheme is mostly independent of the choice of replication scheme, we will ignore replication in this chapter for the sake of simplicity.

## 1. Sharding and Partitioning

What we call a shard in this chapter has many different names depending on which software you're using: it's called a partition in Kafka, a range in CockroachDB, a region in HBase and TiDB, a tablet in Bigtable and YugabyteDB, a vnode in Cassandra, ScyllaDB, and Riak, and a vBucket in Couchbase, to name just a few.

Some databases treat partitions and shards as two distinct concepts. For example, in PostgreSQL, partitioning is a way of splitting a large table into several files that are stored on the same machine (which has several advantages, such as making it very fast to delete an entire partition), whereas sharding splits a dataset across multiple machines. In many other systems, partitioning is just another word for sharding.

> **üí° Insight**
>
> The terminology is confusing, but the core idea is simple: break a big dataset into smaller pieces. Whether you call them shards, partitions, tablets, or vnodes, the fundamental challenge is the same‚Äîhow do you split the data evenly while maintaining fast lookups?

While partitioning is quite descriptive, the term sharding is perhaps surprising. According to one theory, the term arose from the online role-play game Ultima Online, in which a magic crystal was shattered into pieces, and each of those shards refracted a copy of the game world. The term shard thus came to mean one of a set of parallel game servers, and later was carried over to databases. Another theory is that shard was originally an acronym of System for Highly Available Replicated Data‚Äîreportedly a 1980s database, details of which are lost to history.

By the way, partitioning has nothing to do with network partitions (netsplits), a type of fault in the network between nodes. We will discuss such faults in Chapter 9.

## 2. Pros and Cons of Sharding

**In plain English:** Sharding is like choosing between one massive warehouse or many smaller warehouses distributed across the country. The distributed approach handles more volume, but adds complexity in tracking where everything is stored.

**In technical terms:** The primary reason for sharding a database is scalability: it's a solution if the volume of data or the write throughput has become too great for a single node to handle, as it allows you to spread that data and those writes across multiple nodes. (If read throughput is the problem, you don't necessarily need sharding‚Äîyou can use read scaling as discussed in Chapter 6.)

**Why it matters:** Sharding is one of the main tools we have for achieving horizontal scaling (a scale-out architecture): that is, allowing a system to grow its capacity not by moving to a bigger machine, but by adding more (smaller) machines. If you can divide the workload such that each shard handles a roughly equal share, you can then assign those shards to different machines in order to process their data and queries in parallel.

<CardGrid columns={2} cards={[
  {
    title: "When Sharding Helps",
    icon: "‚úÖ",
    color: colors.green,
    items: [
      "Data volume exceeds single machine capacity",
      "Write throughput overwhelms one server",
      "Need horizontal scaling for cost efficiency",
      "Data naturally partitions (e.g., by tenant)"
    ]
  },
  {
    title: "When to Avoid Sharding",
    icon: "‚ö†Ô∏è",
    color: colors.orange,
    items: [
      "Single machine can handle the load",
      "Queries frequently join across shards",
      "Distributed transactions required",
      "Team lacks operational experience"
    ]
  }
]} />

While replication is useful at both small and large scale, because it enables fault tolerance and offline operation, sharding is a heavyweight solution that is mostly relevant at large scale. If your data volume and write throughput are such that you can process them on a single machine (and a single machine can do a lot nowadays!), it's often better to avoid sharding and stick with a single-shard database.

The reason for this recommendation is that sharding often adds complexity: you typically have to decide which records to put in which shard by choosing a partition key; all records with the same partition key are placed in the same shard. This choice matters because accessing a record is fast if you know which shard it's in, but if you don't know the shard you have to do an inefficient search across all shards, and the sharding scheme is difficult to change.

> **üí° Insight**
>
> The "know your shard" principle is crucial. When you query by partition key (e.g., `user_id = 123`), the system routes directly to one shard. But when you query without it (e.g., "find all users named Alice"), you must scatter the query to all shards and gather results‚Äîpotentially 100x slower.

Thus, sharding often works well for key-value data, where you can easily shard by key, but it's harder with relational data where you may want to search by a secondary index, or join records that may be distributed across different shards. We will discuss this further in "Sharding and Secondary Indexes".

Another problem with sharding is that a write may need to update related records in several different shards. While transactions on a single node are quite common (see Chapter 8), ensuring consistency across multiple shards requires a distributed transaction. As we shall see in Chapter 8, distributed transactions are available in some databases, but they are usually much slower than single-node transactions, may become a bottleneck for the system as a whole, and some systems don't support them at all.

Some systems use sharding even on a single machine, typically running one single-threaded process per CPU core to make use of the parallelism in the CPU, or to take advantage of a nonuniform memory access (NUMA) architecture in which some banks of memory are closer to one CPU than to others. For example, Redis, VoltDB, and FoundationDB use one process per core, and rely on sharding to spread load across CPU cores in the same machine.

## 3. Sharding for Multitenancy

**In plain English:** Think of a multitenant system like an apartment building. Each tenant (customer) has their own apartment (shard), with privacy from neighbors and independent utilities. If one tenant's plumbing breaks, it doesn't flood everyone else's apartment.

**In technical terms:** Software as a Service (SaaS) products and cloud services are often multitenant, where each tenant is a customer. Multiple users may have logins on the same tenant, but each tenant has a self-contained dataset that is separate from other tenants. For example, in an email marketing service, each business that signs up is typically a separate tenant, since one business's newsletter signups, delivery data etc. are separate from those of other businesses.

**Why it matters:** Using sharding for multitenancy provides strong isolation guarantees‚Äîbugs, performance problems, or security issues in one tenant's data are less likely to affect others. This architectural boundary makes the system more resilient.

Sometimes sharding is used to implement multitenant systems: either each tenant is given a separate shard, or multiple small tenants may be grouped together into a larger shard. These shards might be physically separate databases (which we previously touched on in "Embedded storage engines"), or separately manageable portions of a larger logical database. Using sharding for multitenancy has several advantages:

### 3.1. Resource isolation

If one tenant performs a computationally expensive operation, it is less likely that other tenants' performance will be affected if they are running on different shards.

### 3.2. Permission isolation

If there is a bug in your access control logic, it's less likely that you will accidentally give one tenant access to another tenant's data if those tenants' datasets are stored physically separately from each other.

### 3.3. Cell-based architecture

**In plain English:** A cell-based architecture is like having separate mini data centers for different customer groups. If one mini data center has a fire, only customers in that cell are affected‚Äîeveryone else keeps working.

**In technical terms:** You can apply sharding not only at the data storage level, but also for the services running your application code. In a cell-based architecture, the services and storage for a particular set of tenants are grouped into a self-contained cell, and different cells are set up such that they can run largely independently from each other. This approach provides fault isolation: that is, a fault in one cell remains limited to that cell, and tenants in other cells are not affected.

**Why it matters:** Cell-based architectures prevent cascading failures. When a deployment goes wrong or a bug appears, the blast radius is limited to one cell instead of bringing down the entire system.

### 3.4. Per-tenant backup and restore

Backing up each tenant's shard separately makes it possible to restore a tenant's state from a backup without affecting other tenants, which can be useful in case the tenant accidentally deletes or overwrites important data.

### 3.5. Regulatory compliance

Data privacy regulation such as the GDPR gives individuals the right to access and delete all data stored about them. If each person's data is stored in a separate shard, this translates into simple data export and deletion operations on their shard.

### 3.6. Data residence

If a particular tenant's data needs to be stored in a particular jurisdiction in order to comply with data residency laws, a region-aware database can allow you to assign that tenant's shard to a particular region.

### 3.7. Gradual schema rollout

Schema migrations (previously discussed in "Schema flexibility in the document model") can be rolled out gradually, one tenant at a time. This reduces risk, as you can detect problems before they affect all tenants, but it can be difficult to do transactionally.

<CardGrid columns={3} cards={[
  {
    title: "Challenge: Large Tenants",
    icon: "üìä",
    color: colors.orange,
    description: "Single tenant too big for one machine requires within-tenant sharding"
  },
  {
    title: "Challenge: Small Tenants",
    icon: "üîç",
    color: colors.orange,
    description: "Many tiny tenants create overhead; grouping them complicates rebalancing"
  },
  {
    title: "Challenge: Cross-Tenant Features",
    icon: "üîó",
    color: colors.orange,
    description: "Features connecting multiple tenants become harder with shard boundaries"
  }
]} />

The main challenges around using sharding for multitenancy are:

It assumes that each individual tenant is small enough to fit on a single node. If that is not the case, and you have a single tenant that's too big for one machine, you would need to additionally perform sharding within a single tenant, which brings us back to the topic of sharding for scalability.

If you have many small tenants, then creating a separate shard for each one may incur too much overhead. You could group several small tenants together into a bigger shard, but then you have the problem of how you move tenants from one shard to another as they grow.

If you ever need to support features that connect data across multiple tenants, these become harder to implement if you need to join data across multiple shards.

## 4. Sharding of Key-Value Data

Say you have a large amount of data, and you want to shard it. How do you decide which records to store on which nodes?

**In plain English:** Imagine you have 10 million user records to spread across 10 servers. Ideally, each server gets exactly 1 million users, and when you look up a user, you can quickly figure out which server has their data.

**In technical terms:** Our goal with sharding is to spread the data and the query load evenly across nodes. If every node takes a fair share, then‚Äîin theory‚Äî10 nodes should be able to handle 10 times as much data and 10 times the read and write throughput of a single node (ignoring replication). Moreover, if we add or remove a node, we want to be able to rebalance the load so that it is evenly distributed across the 11 (when adding) or the remaining 9 (when removing) nodes.

**Why it matters:** Uneven distribution (skew) wastes resources. If 90% of your data lands on one shard, that shard becomes the bottleneck while the other 9 sit mostly idle.

If the sharding is unfair, so that some shards have more data or queries than others, we call it skewed. The presence of skew makes sharding much less effective. In an extreme case, all the load could end up on one shard, so 9 out of 10 nodes are idle and your bottleneck is the single busy node. A shard with disproportionately high load is called a hot shard or hot spot. If there's one key with a particularly high load (e.g., a celebrity in a social network), we call it a hot key.

Therefore we need an algorithm that takes as input the partition key of a record, and tells us which shard that record is in. In a key-value store the partition key is usually the key, or the first part of the key. In a relational model the partition key might be some column of a table (not necessarily its primary key). That algorithm needs to be amenable to rebalancing in order to relieve hot spots.

## 5. Sharding by Key Range

**In plain English:** Key range sharding is like organizing a library by book title. Books starting with A-D go on shelf 1, E-K on shelf 2, and so on. If you want "Harry Potter," you know exactly which shelf to check.

**In technical terms:** One way of sharding is to assign a contiguous range of partition keys (from some minimum to some maximum) to each shard, like the volumes of a paper encyclopedia. In this example, an entry's partition key is its title. If you want to look up the entry for a particular title, you can easily determine which shard contains that entry by finding the volume whose key range contains the title you're looking for, and thus pick the correct book off the shelf.

**Why it matters:** Range queries are extremely efficient with key-range sharding. If you partition by timestamp, queries like "get all events from March 2025" only touch one shard instead of scanning all shards.

<DiagramContainer title="Encyclopedia Volumes (Key Range Sharding)">
  <Row>
    <Box color={colors.blue} size="lg">Vol 1<br/>A-B</Box>
    <Box color={colors.purple} size="lg">Vol 2<br/>C-D</Box>
    <Box color={colors.green} size="lg">Vol 3<br/>E-G</Box>
    <Box color={colors.orange} size="lg">Vol 12<br/>T-Z</Box>
  </Row>
  <Box variant="subtle" color={colors.slate}>
    Ranges sized to balance data volume, not alphabetically even
  </Box>
</DiagramContainer>

The ranges of keys are not necessarily evenly spaced, because your data may not be evenly distributed. For example, in the encyclopedia illustration, volume 1 contains words starting with A and B, but volume 12 contains words starting with T, U, V, W, X, Y, and Z. Simply having one volume per two letters of the alphabet would lead to some volumes being much bigger than others. In order to distribute the data evenly, the shard boundaries need to adapt to the data.

The shard boundaries might be chosen manually by an administrator, or the database can choose them automatically. Manual key-range sharding is used by Vitess (a sharding layer for MySQL), for example; the automatic variant is used by Bigtable, its open source equivalent HBase, the range-based sharding option in MongoDB, CockroachDB, RethinkDB, and FoundationDB. YugabyteDB offers both manual and automatic tablet splitting.

Within each shard, keys are stored in sorted order (e.g., in a B-tree or SSTables, as discussed in Chapter 4). This has the advantage that range scans are easy, and you can treat the key as a concatenated index in order to fetch several related records in one query (see "Multidimensional and Full-Text Indexes"). For example, consider an application that stores data from a network of sensors, where the key is the timestamp of the measurement. Range scans are very useful in this case, because they let you easily fetch, say, all the readings from a particular month.

> **üí° Insight**
>
> Key range sharding has a critical vulnerability: sequential writes create hot spots. If you write logs with timestamp keys, all writes go to the "current time" shard while other shards sit idle. The solution: prefix timestamps with another dimension like `sensor_id`, spreading writes across shards at the cost of harder range queries.

A downside of key range sharding is that you can easily get a hot shard if there are a lot of writes to nearby keys. For example, if the key is a timestamp, then the shards correspond to ranges of time‚Äîe.g., one shard per month. Unfortunately, if you write data from the sensors to the database as the measurements happen, all the writes end up going to the same shard (the one for this month), so that shard can be overloaded with writes while others sit idle.

To avoid this problem in the sensor database, you need to use something other than the timestamp as the first element of the key. For example, you could prefix each timestamp with the sensor ID so that the key ordering is first by sensor ID and then by timestamp. Assuming you have many sensors active at the same time, the write load will end up more evenly spread across the shards. The downside is that when you want to fetch the values of multiple sensors within a time range, you now need to perform a separate range query for each sensor.

### 5.1. Rebalancing key-range sharded data

When you first set up your database, there are no key ranges to split into shards. Some databases, such as HBase and MongoDB, allow you to configure an initial set of shards on an empty database, which is called pre-splitting. This requires that you already have some idea of what the key distribution is going to look like, so that you can choose appropriate key range boundaries.

Later on, as your data volume and write throughput grow, a system with key-range sharding grows by splitting an existing shard into two or more smaller shards, each of which holds a contiguous sub-range of the original shard's key range. The resulting smaller shards can then be distributed across multiple nodes. If large amounts of data are deleted, you may also need to merge several adjacent shards that have become small into one bigger one. This process is similar to what happens at the top level of a B-tree (see "B-Trees").

<ProcessFlow
  direction="horizontal"
  steps={[
    {
      title: "Initial Shard",
      description: "Shard grows to 10GB (threshold reached)",
      color: colors.orange,
      icon: "üì¶"
    },
    {
      title: "Split Triggered",
      description: "System decides to split at median key",
      color: colors.purple,
      icon: "‚úÇÔ∏è"
    },
    {
      title: "Two New Shards",
      description: "5GB each, distributed to different nodes",
      color: colors.green,
      icon: "‚úÖ"
    }
  ]}
/>

With databases that manage shard boundaries automatically, a shard split is typically triggered by:

the shard reaching a configured size (for example, on HBase, the default is 10 GB), or

in some systems, the write throughput being persistently above some threshold. Thus, a hot shard may be split even if it is not storing a lot of data, so that its write load can be distributed more uniformly.

An advantage of key-range sharding is that the number of shards adapts to the data volume. If there is only a small amount of data, a small number of shards is sufficient, so overheads are small; if there is a huge amount of data, the size of each individual shard is limited to a configurable maximum.

A downside of this approach is that splitting a shard is an expensive operation, since it requires all of its data to be rewritten into new files, similarly to a compaction in a log-structured storage engine. A shard that needs splitting is often also one that is under high load, and the cost of splitting can exacerbate that load, risking it becoming overloaded.

## 6. Sharding by Hash of Key

**In plain English:** Hash sharding is like shuffling a deck of cards before dealing them out. Even if the cards were originally ordered, shuffling distributes them randomly. Hashing does the same‚Äîit takes any key (even sequential ones like timestamps) and scrambles them into a uniform distribution.

**In technical terms:** Key-range sharding is useful if you want records with nearby (but different) partition keys to be grouped into the same shard; for example, this might be the case with timestamps. If you don't care whether partition keys are near each other (e.g., if they are tenant IDs in a multitenant application), a common approach is to first hash the partition key before mapping it to a shard.

**Why it matters:** Hashing destroys the ordering of keys, making range queries inefficient, but it distributes data much more evenly than key-range sharding. This trades query flexibility for load balancing.

A good hash function takes skewed data and makes it uniformly distributed. Say you have a 32-bit hash function that takes a string. Whenever you give it a new string, it returns a seemingly random number between 0 and 2¬≥¬≤ ‚àí 1. Even if the input strings are very similar, their hashes are evenly distributed across that range of numbers (but the same input always produces the same output).

For sharding purposes, the hash function need not be cryptographically strong: for example, MongoDB uses MD5, whereas Cassandra and ScyllaDB use Murmur3. Many programming languages have simple hash functions built in (as they are used for hash tables), but they may not be suitable for sharding: for example, in Java's Object.hashCode() and Ruby's Object#hash, the same key may have a different hash value in different processes, making them unsuitable for sharding.

### 6.1. Hash modulo number of nodes

**In plain English:** Taking `hash(key) % 10` is like rolling a 10-sided die for each key‚Äîeach key randomly gets assigned a number from 0-9, determining which of 10 nodes stores it.

**In technical terms:** Once you have hashed the key, how do you choose which shard to store it in? Maybe your first thought is to take the hash value modulo the number of nodes in the system (using the % operator in many programming languages). For example, hash(key) % 10 would return a number between 0 and 9 (if we write the hash as a decimal number, the hash % 10 would be the last digit). If we have 10 nodes, numbered 0 to 9, that seems like an easy way of assigning each key to a node.

**Why it matters:** The mod N approach seems simple but fails catastrophically during rebalancing. When you add or remove even one node, nearly every key moves to a different node, creating a massive data migration.

<DiagramContainer title="Hash Modulo Rebalancing Problem">
  <Column gap="lg">
    <Group title="Before: 3 Nodes" color={colors.blue}>
      <Row>
        <Box color={colors.blue} size="sm">Node 0<br/>hash % 3 = 0</Box>
        <Box color={colors.purple} size="sm">Node 1<br/>hash % 3 = 1</Box>
        <Box color={colors.green} size="sm">Node 2<br/>hash % 3 = 2</Box>
      </Row>
    </Group>
    <Arrow direction="down" label="Add Node 3" />
    <Group title="After: 4 Nodes" color={colors.orange}>
      <Row>
        <Box color={colors.blue} size="sm">Node 0<br/>hash % 4 = 0</Box>
        <Box color={colors.purple} size="sm">Node 1<br/>hash % 4 = 1</Box>
        <Box color={colors.green} size="sm">Node 2<br/>hash % 4 = 2</Box>
        <Box color={colors.orange} size="sm">Node 3<br/>hash % 4 = 3</Box>
      </Row>
    </Group>
    <Box variant="subtle" color={colors.red}>
      ‚ö†Ô∏è Most keys move! hash=6 moves from Node 0 to Node 2, hash=9 moves from Node 0 to Node 1
    </Box>
  </Column>
</DiagramContainer>

The problem with the mod N approach is that if the number of nodes N changes, most of the keys have to be moved from one node to another. The mod N function is easy to compute, but it leads to very inefficient rebalancing because there is a lot of unnecessary movement of records from one node to another. We need an approach that doesn't move data around more than necessary.

### 6.2. Fixed number of shards

**In plain English:** Instead of assigning keys directly to nodes, create many more "buckets" (shards) than you have nodes, then assign multiple buckets to each node. When you add a node, just reassign some buckets‚Äîthe keys inside each bucket don't move.

**In technical terms:** One simple but widely-used solution is to create many more shards than there are nodes, and to assign several shards to each node. For example, a database running on a cluster of 10 nodes may be split into 1,000 shards from the outset so that 100 shards are assigned to each node. A key is then stored in shard number hash(key) % 1,000, and the system separately keeps track of which shard is stored on which node.

**Why it matters:** This approach dramatically reduces rebalancing cost. Adding a node only requires moving entire shards (cheap bulk copies) rather than reshuffling individual keys (expensive random I/O).

<DiagramContainer title="Fixed Shards with Rebalancing">
  <Column gap="lg">
    <Group title="10 Nodes, 1000 Shards" color={colors.blue}>
      <Row>
        <Box color={colors.blue}>Node 1<br/>Shards 0-99</Box>
        <Box color={colors.purple}>Node 2<br/>Shards 100-199</Box>
        <Box color={colors.green}>Node 3<br/>Shards 200-299</Box>
        <Box color={colors.slate} variant="outlined">...</Box>
      </Row>
    </Group>
    <Arrow direction="down" label="Add Node 11" />
    <Group title="11 Nodes, 1000 Shards" color={colors.green}>
      <Row>
        <Box color={colors.blue}>Node 1<br/>Shards 0-90</Box>
        <Box color={colors.purple}>Node 2<br/>Shards 100-190</Box>
        <Box color={colors.orange} size="sm">Node 11<br/>Shards 91-99, 191-199...</Box>
      </Row>
    </Group>
    <Box variant="subtle" color={colors.green}>
      ‚úÖ Only entire shards move, keys stay in same shard number
    </Box>
  </Column>
</DiagramContainer>

Now, if a node is added to the cluster, the system can reassign some of the shards from existing nodes to the new node until they are fairly distributed once again. If a node is removed from the cluster, the same happens in reverse.

In this model, only entire shards are moved between nodes, which is cheaper than splitting shards. The number of shards does not change, nor does the assignment of keys to shards. The only thing that changes is the assignment of shards to nodes. This change of assignment is not immediate‚Äîit takes some time to transfer a large amount of data over the network‚Äîso the old assignment of shards is used for any reads and writes that happen while the transfer is in progress.

> **üí° Insight**
>
> Choosing the right number of shards is an art. Too few shards and you can't scale past a certain number of nodes. Too many shards and overhead becomes significant. A common heuristic: start with 10-100x more shards than initial nodes, allowing room to grow.

It's common to choose the number of shards to be a number that is divisible by many factors, so that the dataset can be evenly split across various different numbers of nodes‚Äînot requiring the number of nodes to be a power of 2, for example. You can even account for mismatched hardware in your cluster: by assigning more shards to nodes that are more powerful, you can make those nodes take a greater share of the load.

This approach to sharding is used in Citus (a sharding layer for PostgreSQL), Riak, Elasticsearch, and Couchbase, among others. It works well as long as you have a good estimate of how many shards you will need when you first create the database. You can then add or remove nodes easily, subject to the limitation that you can't have more nodes than you have shards.

If you find the originally configured number of shards to be wrong‚Äîfor example, if you have reached a scale where you need more nodes than you have shards‚Äîthen an expensive resharding operation is required. It needs to split each shard and write it out to new files, using a lot of additional disk space in the process. Some systems don't allow resharding while concurrently writing to the database, which makes it difficult to change the number of shards without downtime.

Choosing the right number of shards is difficult if the total size of the dataset is highly variable (for example, if it starts small but may grow much larger over time). Since each shard contains a fixed fraction of the total data, the size of each shard grows proportionally to the total amount of data in the cluster. If shards are very large, rebalancing and recovery from node failures become expensive. But if shards are too small, they incur too much overhead. The best performance is achieved when the size of shards is "just right," neither too big nor too small, which can be hard to achieve if the number of shards is fixed but the dataset size varies.

### 6.3. Sharding by hash range

**In plain English:** Combine the best of both worlds‚Äîuse hash to scramble keys (avoiding hot spots), then use ranges of hash values instead of individual shards. This lets you split and merge as needed while maintaining even distribution.

**In technical terms:** If the required number of shards can't be predicted in advance, it's better to use a scheme in which the number of shards can adapt easily to the workload. The aforementioned key-range sharding scheme has this property, but it has a risk of hot spots when there are a lot of writes to nearby keys. One solution is to combine key-range sharding with a hash function so that each shard contains a range of hash values rather than a range of keys.

**Why it matters:** Hash-range sharding adapts to data growth like key-range sharding, but distributes load evenly like hash sharding. The tradeoff: you lose efficient range queries over the original keys.

<DiagramContainer title="Hash Range Sharding">
  <Column>
    <Box variant="subtle" color={colors.slate}>
      16-bit hash: 0 to 65,535
    </Box>
    <Row>
      <Box color={colors.blue}>Shard 0<br/>0-16,383</Box>
      <Box color={colors.purple}>Shard 1<br/>16,384-32,767</Box>
      <Box color={colors.green}>Shard 2<br/>32,768-49,151</Box>
      <Box color={colors.orange}>Shard 3<br/>49,152-65,535</Box>
    </Row>
    <Box variant="subtle" color={colors.green}>
      Keys "2025-01-15" and "2025-01-16" have random hashes ‚Üí land in different shards
    </Box>
  </Column>
</DiagramContainer>

Even if the input keys are very similar (e.g., consecutive timestamps), their hashes are uniformly distributed across that range. We can then assign a range of hash values to each shard: for example, values between 0 and 16,383 to shard 0, values between 16,384 and 32,767 to shard 1, and so on.

Like with key-range sharding, a shard in hash-range sharding can be split when it becomes too big or too heavily loaded. This is still an expensive operation, but it can happen as needed, so the number of shards adapts to the volume of data rather than being fixed in advance.

The downside compared to key-range sharding is that range queries over the partition key are not efficient, as keys in the range are now scattered across all the shards. However, if keys consist of two or more columns, and the partition key is only the first of these columns, you can still perform efficient range queries over the second and later columns: as long as all records in the range query have the same partition key, they will be in the same shard.

## 7. Partitioning and Range Queries in Data Warehouses

Data warehouses such as BigQuery, Snowflake, and Delta Lake support a similar indexing approach, though the terminology differs. In BigQuery, for example, the partition key determines which partition a record resides in while "cluster columns" determine how records are sorted within the partition. Snowflake assigns records to "micro-partitions" automatically, but allows users to define cluster keys for a table. Delta Lake supports both manual and automatic partition assignment, and supports cluster keys. Clustering data not only improves range scan performance, but can improve compression and filtering performance as well.

Hash-range sharding is used in YugabyteDB and DynamoDB, and is an option in MongoDB. Cassandra and ScyllaDB use a variant of this approach that is illustrated below: the space of hash values is split into a number of ranges proportional to the number of nodes (3 ranges per node in the example, but actual numbers are 8 per node in Cassandra by default, and 256 per node in ScyllaDB), with random boundaries between those ranges. This means some ranges are bigger than others, but by having multiple ranges per node those imbalances tend to even out.

<DiagramContainer title="Cassandra/ScyllaDB Hash Range Distribution">
  <Column>
    <Box variant="subtle" color={colors.slate}>
      Hash space 0-1023 split into ranges with random boundaries
    </Box>
    <Row wrap={true}>
      <Box color={colors.blue} size="sm">Node 1<br/>0-125</Box>
      <Box color={colors.blue} size="sm">Node 1<br/>340-502</Box>
      <Box color={colors.blue} size="sm">Node 1<br/>780-890</Box>
      <Box color={colors.purple} size="sm">Node 2<br/>126-339</Box>
      <Box color={colors.purple} size="sm">Node 2<br/>503-600</Box>
      <Box color={colors.purple} size="sm">Node 2<br/>891-1023</Box>
      <Box color={colors.green} size="sm">Node 3<br/>601-779</Box>
    </Row>
    <Box variant="subtle" color={colors.green}>
      Multiple ranges per node balance out size differences
    </Box>
  </Column>
</DiagramContainer>

When nodes are added or removed, range boundaries are added and removed, and shards are split or merged accordingly. In the example, when node 3 is added, node 1 transfers parts of two of its ranges to node 3, and node 2 transfers part of one of its ranges to node 3. This has the effect of giving the new node an approximately fair share of the dataset, without transferring more data than necessary from one node to another.

### 7.1. Consistent hashing

**In plain English:** Consistent hashing is like a smart shuffling algorithm that tries to keep cards in the same pile when you add or remove piles, minimizing how many cards need to move.

**In technical terms:** A consistent hashing algorithm is a hash function that maps keys to a specified number of shards in a way that satisfies two properties:

the number of keys mapped to each shard is roughly equal, and

when the number of shards changes, as few keys as possible are moved from one shard to another.

**Why it matters:** The name is confusing‚Äî"consistent" here means "stable assignment," not "replica consistency" (Chapter 6) or "ACID consistency" (Chapter 8). The key benefit is minimizing data movement during rebalancing.

> **üí° Insight**
>
> Despite its name, consistent hashing doesn't guarantee consistency in the ACID sense. It simply tries to keep a key in the same shard across cluster changes. Multiple algorithms achieve this goal‚ÄîCassandra's token ring, rendezvous hashing, and jump consistent hash are all "consistent hashing" variants with different tradeoffs.

Note that consistent here has nothing to do with replica consistency (see Chapter 6) or ACID consistency (see Chapter 8), but rather describes the tendency of a key to stay in the same shard as much as possible.

The sharding algorithm used by Cassandra and ScyllaDB is similar to the original definition of consistent hashing, but several other consistent hashing algorithms have also been proposed, such as highest random weight, also known as rendezvous hashing, and jump consistent hash. With Cassandra's algorithm, if one node is added, a small number of existing shards are split into sub-ranges; on the other hand, with rendezvous and jump consistent hashes, the new node is assigned individual keys that were previously scattered across all of the other nodes. Which one is preferable depends on the application.

## 8. Skewed Workloads and Relieving Hot Spots

**In plain English:** Imagine a celebrity tweets and gets 10 million replies. Even perfect sharding can't help if all those writes go to a single key (the celebrity's tweet ID). You need application-level tricks to split the hot key itself.

**In technical terms:** Consistent hashing ensures that keys are uniformly distributed across nodes, but that doesn't mean that the actual load is uniformly distributed. If the workload is highly skewed‚Äîthat is, the amount of data under some partition keys is much greater than other keys, or if the rate of requests to some keys is much higher than to others‚Äîyou can still end up with some servers being overloaded while others sit almost idle.

**Why it matters:** Hot keys represent a fundamental limit of sharding. No amount of additional nodes helps if all traffic goes to a single key. The solution requires splitting the key itself, which adds complexity to both writes and reads.

For example, on a social media site, a celebrity user with millions of followers may cause a storm of activity when they do something. This event can result in a large volume of reads and writes to the same key (where the partition key is perhaps the user ID of the celebrity, or the ID of the action that people are commenting on).

<ProcessFlow
  direction="horizontal"
  steps={[
    {
      title: "Detect Hot Key",
      description: "Celebrity tweet getting millions of writes",
      color: colors.red,
      icon: "üî•"
    },
    {
      title: "Split the Key",
      description: "Append random number 0-99 to key",
      color: colors.orange,
      icon: "‚úÇÔ∏è"
    },
    {
      title: "Distribute Writes",
      description: "100 keys across different shards",
      color: colors.green,
      icon: "‚úÖ"
    },
    {
      title: "Read All Variants",
      description: "Query all 100 keys and merge results",
      color: colors.blue,
      icon: "üîÑ"
    }
  ]}
/>

In such situations, a more flexible sharding policy is required. A system that defines shards based on ranges of keys (or ranges of hashes) makes it possible to put an individual hot key in a shard by its own, and perhaps even assigning it a dedicated machine.

It's also possible to compensate for skew at the application level. For example, if one key is known to be very hot, a simple technique is to add a random number to the beginning or end of the key. Just a two-digit decimal random number would split the writes to the key evenly across 100 different keys, allowing those keys to be distributed to different shards.

However, having split the writes across different keys, any reads now have to do additional work, as they have to read the data from all 100 keys and combine it. The volume of reads to each shard of the hot key is not reduced; only the write load is split. This technique also requires additional bookkeeping: it only makes sense to append the random number for the small number of hot keys; for the vast majority of keys with low write throughput this would be unnecessary overhead. Thus, you also need some way of keeping track of which keys are being split, and a process for converting a regular key into a specially-managed hot key.

> **üí° Insight**
>
> Hot key handling is often application-specific. A social media platform might cache celebrity posts differently than regular posts. A voting system might use a different aggregation strategy for popular items. The database can detect hot keys, but the application usually needs to implement the mitigation.

The problem is further compounded by change of load over time: for example, a particular social media post that has gone viral may experience high load for a couple of days, but thereafter it's likely to calm down again. Moreover, some keys may be hot for writes while others are hot for reads, necessitating different strategies for handling them.

Some systems (especially cloud services designed for large scale) have automated approaches for dealing with hot shards; for example, Amazon calls it heat management or adaptive capacity. The details of how these systems work go beyond the scope of this book.

## 9. Operations: Automatic or Manual Rebalancing

**In plain English:** Should your database automatically split shards and move them around, or should a human operator make those decisions? It's the difference between autopilot and manual flying‚Äîautopilot is convenient but can make dangerous decisions when things go wrong.

**In technical terms:** There is one important question with regard to rebalancing that we have glossed over: does the splitting of shards and rebalancing happen automatically or manually?

**Why it matters:** Automatic rebalancing can save operational work but risks cascading failures. If one slow node triggers automatic rebalancing, that adds load to other nodes, potentially triggering more rebalancing, spiraling into a cluster-wide outage. Human oversight can prevent such death spirals.

Some systems automatically decide when to split shards and when to move them from one node to another, without any human interaction, while others leave sharding to be explicitly configured by an administrator. There is also a middle ground: for example, Couchbase and Riak generate a suggested shard assignment automatically, but require an administrator to commit it before it takes effect.

<CardGrid columns={2} cards={[
  {
    title: "Automatic Rebalancing",
    icon: "ü§ñ",
    color: colors.blue,
    items: [
      "Pro: Less operational work",
      "Pro: Can auto-scale with load",
      "Con: Unpredictable behavior",
      "Con: Risk of cascading failures"
    ]
  },
  {
    title: "Manual Rebalancing",
    icon: "üë§",
    color: colors.green,
    items: [
      "Pro: Predictable operations",
      "Pro: Human prevents mistakes",
      "Con: Slower to respond",
      "Con: Requires operator expertise"
    ]
  }
]} />

Fully automated rebalancing can be convenient, because there is less operational work to do for normal maintenance, and such systems can even auto-scale to adapt to changes in workload. Cloud databases such as DynamoDB are promoted as being able to automatically add and remove shards to adapt to big increases or decreases of load within a matter of minutes.

However, automatic shard management can also be unpredictable. Rebalancing is an expensive operation, because it requires rerouting requests and moving a large amount of data from one node to another. If it is not done carefully, this process can overload the network or the nodes, and it might harm the performance of other requests. The system must continue processing writes while the rebalancing is in progress; if a system is near its maximum write throughput, the shard-splitting process might not even be able to keep up with the rate of incoming writes.

Such automation can be dangerous in combination with automatic failure detection. For example, say one node is overloaded and is temporarily slow to respond to requests. The other nodes conclude that the overloaded node is dead, and automatically rebalance the cluster to move load away from it. This puts additional load on other nodes and the network, making the situation worse. There is a risk of causing a cascading failure where other nodes become overloaded and are also falsely suspected of being down.

For that reason, it can be a good thing to have a human in the loop for rebalancing. It's slower than a fully automatic process, but it can help prevent operational surprises.

## 10. Request Routing

**In plain English:** When a client wants to read key "user:12345", how does it know which server to contact? This is like calling directory assistance‚Äîsomeone needs to know the mapping from keys to servers and help route the call.

**In technical terms:** We have discussed how to shard a dataset across multiple nodes, and how to rebalance those shards as nodes are added or removed. Now let's move on to the question: if you want to read or write a particular key, how do you know which node‚Äîi.e., which IP address and port number‚Äîyou need to connect to?

**Why it matters:** Request routing is the glue that makes sharding work in practice. Without it, clients would need to query all nodes to find their data‚Äîdefeating the purpose of sharding.

We call this problem request routing, and it's very similar to service discovery, which we previously discussed in "Load balancers, service discovery, and service meshes". The biggest difference between the two is that with services running application code, each instance is usually stateless, and a load balancer can send a request to any of the instances. With sharded databases, a request for a key can only be handled by a node that is a replica for the shard containing that key.

<DiagramContainer title="Three Request Routing Approaches">
  <Column gap="lg">
    <Group title="Approach 1: Client ‚Üí Any Node ‚Üí Correct Node" color={colors.blue}>
      <Row>
        <Box color={colors.slate} variant="outlined">Client</Box>
        <Arrow />
        <Box color={colors.blue}>Any Node</Box>
        <Arrow label="forward" />
        <Box color={colors.green}>Correct Node</Box>
      </Row>
    </Group>
    <Group title="Approach 2: Client ‚Üí Routing Tier ‚Üí Correct Node" color={colors.purple}>
      <Row>
        <Box color={colors.slate} variant="outlined">Client</Box>
        <Arrow />
        <Box color={colors.purple}>Routing Tier</Box>
        <Arrow />
        <Box color={colors.green}>Correct Node</Box>
      </Row>
    </Group>
    <Group title="Approach 3: Smart Client ‚Üí Correct Node" color={colors.green}>
      <Row>
        <Box color={colors.green} variant="outlined">Smart Client</Box>
        <Arrow label="direct" />
        <Box color={colors.green}>Correct Node</Box>
      </Row>
    </Group>
  </Column>
</DiagramContainer>

This means that request routing has to be aware of the assignment from keys to shards, and from shards to nodes. On a high level, there are a few different approaches to this problem:

Allow clients to contact any node (e.g., via a round-robin load balancer). If that node coincidentally owns the shard to which the request applies, it can handle the request directly; otherwise, it forwards the request to the appropriate node, receives the reply, and passes the reply along to the client.

Send all requests from clients to a routing tier first, which determines the node that should handle each request and forwards it accordingly. This routing tier does not itself handle any requests; it only acts as a shard-aware load balancer.

Require that clients be aware of the sharding and the assignment of shards to nodes. In this case, a client can connect directly to the appropriate node, without any intermediary.

In all cases, there are some key problems:

Who decides which shard should live on which node? It's simplest to have a single coordinator making that decision, but in that case how do you make it fault-tolerant in case the node running the coordinator goes down? And if the coordinator role can failover to another node, how do you prevent a split-brain situation (see "Handling Node Outages") where two different coordinators make contradictory shard assignments?

How does the component performing the routing (which may be one of the nodes, or the routing tier, or the client) learn about changes in the assignment of shards to nodes?

While a shard is being moved from one node to another, there is a cutover period during which the new node has taken over, but requests to the old node may still be in flight. How do you handle those?

> **üí° Insight**
>
> Coordination services like ZooKeeper solve the routing problem by providing a single source of truth for shard assignments. All routing decisions (whether in clients, nodes, or routing tier) subscribe to ZooKeeper to learn about changes. This is essentially distributed consensus (Chapter 10) in action.

<DiagramContainer title="ZooKeeper-Based Routing">
  <Column>
    <Box color={colors.orange}>ZooKeeper Cluster<br/>(consensus-based coordination)</Box>
    <Row>
      <Arrow direction="down" label="subscribe" />
      <Arrow direction="down" label="subscribe" />
      <Arrow direction="down" label="subscribe" />
    </Row>
    <Row>
      <Box color={colors.blue} size="sm">Routing Tier</Box>
      <Box color={colors.purple} size="sm">Node 1</Box>
      <Box color={colors.green} size="sm">Node 2</Box>
    </Row>
    <Box variant="subtle" color={colors.green}>
      All components get real-time updates when shard assignments change
    </Box>
  </Column>
</DiagramContainer>

Many distributed data systems rely on a separate coordination service such as ZooKeeper or etcd to keep track of shard assignments. They use consensus algorithms (see Chapter 10) to provide fault tolerance and protection against split-brain. Each node registers itself in ZooKeeper, and ZooKeeper maintains the authoritative mapping of shards to nodes. Other actors, such as the routing tier or the sharding-aware client, can subscribe to this information in ZooKeeper. Whenever a shard changes ownership, or a node is added or removed, ZooKeeper notifies the routing tier so that it can keep its routing information up to date.

For example, HBase and SolrCloud use ZooKeeper to manage shard assignment, and Kubernetes uses etcd to keep track of which service instance is running where. MongoDB has a similar architecture, but it relies on its own config server implementation and mongos daemons as the routing tier. Kafka, YugabyteDB, and TiDB use built-in implementations of the Raft consensus protocol to perform this coordination function.

Cassandra, ScyllaDB, and Riak take a different approach: they use a gossip protocol among the nodes to disseminate any changes in cluster state. This provides much weaker consistency than a consensus protocol; it is possible to have split brain, in which different parts of the cluster have different node assignments for the same shard. Leaderless databases can tolerate this because they generally make weak consistency guarantees anyway (see "Limitations of Quorum Consistency").

When using a routing tier or when sending requests to a random node, clients still need to find the IP addresses to connect to. These are not as fast-changing as the assignment of shards to nodes, so it is often sufficient to use DNS for this purpose.

This discussion of request routing has focused on finding the shard for an individual key, which is most relevant for sharded OLTP databases. Analytic databases often use sharding as well, but they typically have a very different kind of query execution: rather than executing in a single shard, a query typically needs to aggregate and join data from many different shards in parallel. We will discuss techniques for such parallel query execution in Chapter 11.

## 11. Sharding and Secondary Indexes

**In plain English:** Primary key lookups are easy‚Äîyou know the key, you know the shard. But what about queries like "find all red cars"? The red cars are scattered across all shards. Secondary indexes on sharded databases face a fundamental tradeoff: local or global?

**In technical terms:** The sharding schemes we have discussed so far rely on the client knowing the partition key for any record it wants to access. This is most easily done in a key-value data model, where the partition key is the first part of the primary key (or the entire primary key), and so we can use the partition key to determine the shard, and thus route reads and writes to the node that is responsible for that key.

**Why it matters:** Secondary indexes don't map neatly to shards, creating a fundamental challenge. Local indexes make writes fast but reads slow (scatter-gather). Global indexes make reads fast but writes slow (distributed updates). There's no free lunch.

The situation becomes more complicated if secondary indexes are involved (see also "Multi-Column and Secondary Indexes"). A secondary index usually doesn't identify a record uniquely but rather is a way of searching for occurrences of a particular value: find all actions by user 123, find all articles containing the word hogwash, find all cars whose color is red, and so on.

Key-value stores often don't have secondary indexes, but they are the bread and butter of relational databases, they are common in document databases too, and they are the raison d'√™tre of full-text search engines such as Solr and Elasticsearch. The problem with secondary indexes is that they don't map neatly to shards. There are two main approaches to sharding a database with secondary indexes: local and global indexes.

### 11.1. Local Secondary Indexes

**In plain English:** Local indexes are like each library branch maintaining its own card catalog‚Äîit only knows about books in that branch. To find all copies of a book across all branches, you have to call every branch and ask.

**In technical terms:** For example, imagine you are operating a website for selling used cars. Each listing has a unique ID, and you use that ID as partition key for sharding (for example, IDs 0 to 499 in shard 0, IDs 500 to 999 in shard 1, etc.).

If you want to let users search for cars, allowing them to filter by color and by make, you need a secondary index on color and make (in a document database these would be fields; in a relational database they would be columns). If you have declared the index, the database can perform the indexing automatically. For example, whenever a red car is added to the database, the database shard automatically adds its ID to the list of IDs for the index entry color:red. As discussed in Chapter 4, that list of IDs is also called a postings list.

<DiagramContainer title="Local Secondary Indexes (Document-Partitioned)">
  <Row>
    <Group title="Shard 0 (IDs 0-499)" color={colors.blue}>
      <Box color={colors.blue} size="sm" variant="subtle">
        Primary: car IDs 0-499
      </Box>
      <Box color={colors.cyan} size="sm" variant="outlined">
        Index: color:red ‚Üí [12, 87, 234]
      </Box>
      <Box color={colors.cyan} size="sm" variant="outlined">
        Index: make:honda ‚Üí [12, 156]
      </Box>
    </Group>
    <Group title="Shard 1 (IDs 500-999)" color={colors.purple}>
      <Box color={colors.purple} size="sm" variant="subtle">
        Primary: car IDs 500-999
      </Box>
      <Box color={colors.pink} size="sm" variant="outlined">
        Index: color:red ‚Üí [543, 789]
      </Box>
      <Box color={colors.pink} size="sm" variant="outlined">
        Index: make:honda ‚Üí [612]
      </Box>
    </Group>
  </Row>
  <Box variant="subtle" color={colors.orange}>
    Query "find red cars" must hit both shards and merge results [12, 87, 234, 543, 789]
  </Box>
</DiagramContainer>

**Warning:** If your database only supports a key-value model, you might be tempted to implement a secondary index yourself by creating a mapping from values to IDs in application code. If you go down this route, you need to take great care to ensure your indexes remain consistent with the underlying data. Race conditions and intermittent write failures (where some changes were saved but others weren't) can very easily cause the data to go out of sync‚Äîsee "The need for multi-object transactions".

In this indexing approach, each shard is completely separate: each shard maintains its own secondary indexes, covering only the records in that shard. It doesn't care what data is stored in other shards. Whenever you write to the database‚Äîto add, remove, or update a records‚Äîyou only need to deal with the shard that contains the record that you are writing. For that reason, this type of secondary index is known as a local index. In an information retrieval context it is also known as a document-partitioned index.

When reading from a local secondary index, if you already know the partition key of the record you're looking for, you can just perform the search on the appropriate shard. Moreover, if you only want some results, and you don't need all, you can send the request to any shard.

However, if you want all the results and don't know their partition key in advance, you need to send the query to all shards, and combine the results you get back, because the matching records might be scattered across all the shards.

> **üí° Insight**
>
> Local secondary indexes suffer from tail latency amplification. If you query 100 shards in parallel and 99 respond in 10ms but one takes 500ms, your overall query takes 500ms. This is why adding more shards doesn't improve query throughput for secondary index queries‚Äîyou still hit every shard.

This approach to querying a sharded database can make read queries on secondary indexes quite expensive. Even if you query the shards in parallel, it is prone to tail latency amplification (see "Use of Response Time Metrics"). It also limits the scalability of your application: adding more shards lets you store more data, but it doesn't increase your query throughput if every shard has to process every query anyway.

Nevertheless, local secondary indexes are widely used: for example, MongoDB, Riak, Cassandra, Elasticsearch, SolrCloud, and VoltDB all use local secondary indexes.

### 11.2. Global Secondary Indexes

**In plain English:** Global indexes are like a central library catalog that knows about books in all branches. The catalog itself is split across multiple computers (it's too big for one), but each catalog entry points to all locations where that book exists.

**In technical terms:** Rather than each shard having its own, local secondary index, we can construct a global index that covers data in all shards. However, we can't just store that index on one node, since it would likely become a bottleneck and defeat the purpose of sharding. A global index must also be sharded, but it can be sharded differently from the primary key index.

**Why it matters:** Global indexes trade write complexity for read efficiency. A single lookup in the global index tells you exactly which records match (no scatter-gather), but writes must update index shards that may be on different nodes than the data.

<DiagramContainer title="Global Secondary Indexes (Term-Partitioned)">
  <Column gap="lg">
    <Row>
      <Group title="Data Shards" color={colors.blue}>
        <Box color={colors.blue} size="sm">Shard 0<br/>Cars 0-499</Box>
        <Box color={colors.purple} size="sm">Shard 1<br/>Cars 500-999</Box>
      </Group>
    </Row>
    <Row>
      <Group title="Index Shards" color={colors.green}>
        <Box color={colors.green} size="sm">
          Index A-R<br/>
          color:red ‚Üí [12,87,234,543,789]<br/>
          make:honda ‚Üí [12,156,612]
        </Box>
        <Box color={colors.orange} size="sm">
          Index S-Z<br/>
          color:silver ‚Üí [45,234,891]<br/>
          make:toyota ‚Üí [67,234]
        </Box>
      </Group>
    </Row>
  </Column>
  <Box variant="subtle" color={colors.green}>
    Query "find red cars" hits only Index A-R, returns all matching IDs immediately
  </Box>
</DiagramContainer>

The diagram illustrates what this could look like: the IDs of red cars from all shards appear under color:red in the index, but the index is sharded so that colors starting with the letters a to r appear in shard 0 and colors starting with s to z appear in shard 1. The index on the make of car is partitioned similarly (with the shard boundary being between f and h).

This kind of index is also called term-partitioned: recall from "Full-Text Search" that in full-text search, a term is a keyword in a text that you can search for. Here we generalise it to mean any value that you can search for in the secondary index.

The global index uses the term as partition key, so that when you're looking for a particular term or value, you can figure out which shard you need to query. As before, a shard can contain a contiguous range of terms, or you can assign terms to shards based on a hash of the term.

Global indexes have the advantage that a query with a single condition (such as color = red) only needs to read from a single shard to fetch the postings list. However, if you want to fetch records and not just IDs, you still have to read from all the shards that are responsible for those IDs.

If you have multiple search conditions or terms (e.g., searching for cars of a certain color and a certain make, or searching for multiple words occurring in the same text), it's likely that those terms will be assigned to different shards. To compute the logical AND of the two conditions, the system needs to find all the IDs that occur in both of the postings lists. That's no problem if the postings lists are short, but if they are long, it can be slow to send them over the network to compute their intersection.

> **üí° Insight**
>
> Global secondary indexes often lag behind the primary data. When you write a car listing, updating the primary shard is fast (one network hop), but updating the global index shards asynchronously prevents the write from blocking. This means searches may briefly not find newly added items‚Äîeventual consistency strikes again.

Another challenge with global secondary indexes is that writes are more complicated than with local indexes, because writing a single record might affect multiple shards of the index (every term in the document might be on a different shard). This makes it harder to keep the secondary index in sync with the underlying data. One option is to use a distributed transaction to atomically update the shards storing the primary record and its secondary indexes (see Chapter 8).

Global secondary indexes are used by CockroachDB, TiDB, and YugabyteDB; DynamoDB supports both local and global secondary indexes. In the case of DynamoDB, writes are asynchronously reflected in global indexes, so reads from a global index may be stale (similarly to replication lag, as in "Problems with Replication Lag"). Nevertheless, global indexes are useful if read throughput is higher than write throughput, and if the postings lists are not too long.

## 12. Summary

In this chapter we explored different ways of sharding a large dataset into smaller subsets. Sharding is necessary when you have so much data that storing and processing it on a single machine is no longer feasible.

The goal of sharding is to spread the data and query load evenly across multiple machines, avoiding hot spots (nodes with disproportionately high load). This requires choosing a sharding scheme that is appropriate to your data, and rebalancing the shards when nodes are added to or removed from the cluster.

We discussed two main approaches to sharding:

**Key range sharding**, where keys are sorted, and a shard owns all the keys from some minimum up to some maximum. Sorting has the advantage that efficient range queries are possible, but there is a risk of hot spots if the application often accesses keys that are close together in the sorted order.

In this approach, shards are typically rebalanced by splitting the range into two subranges when a shard gets too big.

**Hash sharding**, where a hash function is applied to each key, and a shard owns a range of hash values (or another consistent hashing algorithm may be used to map hashes to shards). This method destroys the ordering of keys, making range queries inefficient, but it may distribute load more evenly.

When sharding by hash, it is common to create a fixed number of shards in advance, to assign several shards to each node, and to move entire shards from one node to another when nodes are added or removed. Splitting shards, like with key ranges, is also possible.

<CardGrid columns={2} cards={[
  {
    title: "Key Range Sharding",
    icon: "üìö",
    color: colors.blue,
    items: [
      "Pro: Efficient range queries",
      "Pro: Dynamic shard count",
      "Con: Sequential writes create hot spots",
      "Con: Expensive to split shards"
    ]
  },
  {
    title: "Hash Sharding",
    icon: "üé≤",
    color: colors.purple,
    items: [
      "Pro: Even load distribution",
      "Pro: Predictable performance",
      "Con: Range queries inefficient",
      "Con: Fixed shard count (or expensive resharding)"
    ]
  }
]} />

It is common to use the first part of the key as the partition key (i.e., to identify the shard), and to sort records within that shard by the rest of the key. That way you can still have efficient range queries among the records with the same partition key.

We also discussed the interaction between sharding and secondary indexes. A secondary index also needs to be sharded, and there are two methods:

**Local secondary indexes**, where the secondary indexes are stored in the same shard as the primary key and value. This means that only a single shard needs to be updated on write, but a lookup of the secondary index requires reading from all shards.

**Global secondary indexes**, which are sharded separately based on the indexed values. An entry in the secondary index may refer to records from all shards of the primary key. When a record is written, several secondary index shards may need to be updated; however, a read of the postings list can be served from a single shard (fetching the actual records still requires reading from multiple shards).

Finally, we discussed techniques for routing queries to the appropriate shard, and how a coordination service is often used to keep track of the assigment of shards to nodes.

> **üí° Insight**
>
> Sharding represents a fundamental tradeoff in distributed systems: you gain scalability at the cost of simplicity. Operations that were simple on a single machine (joins, secondary indexes, transactions) become complex across shards. The key to successful sharding is choosing partition keys that align with your query patterns‚Äîshard by tenant if queries are per-tenant, by user if queries are per-user, by timestamp if queries are time-range scans.

By design, every shard operates mostly independently‚Äîthat's what allows a sharded database to scale to multiple machines. However, operations that need to write to several shards can be problematic: for example, what happens if the write to one shard succeeds, but another fails? We will address that question in the following chapters.

---

**Previous:** [Chapter 6: Replication](chapter06-replication.md) | **Next:** [Chapter 8: Transactions](chapter08-transactions.mdx)

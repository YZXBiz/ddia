---
sidebar_position: 4
title: "Chapter 14. Doing the Right Thing"
description: "Examine the ethical implications and responsibilities when building data-intensive applications"
---

# Chapter 14. Doing the Right Thing

> Feeding AI systems on the world's beauty, ugliness, and cruelty, but expecting it to reflect only the beauty is a fantasy.
>
> _Vinay Uday Prabhu and Abeba Birhane, Large Datasets: A Pyrrhic Win for Computer Vision? (2020)_

## Table of Contents

1. [Introduction](#1-introduction)
   - 1.1. [Data is About People](#11-data-is-about-people)
   - 1.2. [Technology is Not Neutral](#12-technology-is-not-neutral)
2. [Predictive Analytics](#2-predictive-analytics)
   - 2.1. [Algorithmic Prison](#21-algorithmic-prison)
   - 2.2. [Bias and Discrimination](#22-bias-and-discrimination)
   - 2.3. [Responsibility and Accountability](#23-responsibility-and-accountability)
3. [Feedback Loops](#3-feedback-loops)
   - 3.1. [Self-Reinforcing Problems](#31-self-reinforcing-problems)
   - 3.2. [Systems Thinking](#32-systems-thinking)
4. [Privacy and Surveillance](#4-privacy-and-surveillance)
   - 4.1. [The Surveillance Thought Experiment](#41-the-surveillance-thought-experiment)
   - 4.2. [Consent and Freedom of Choice](#42-consent-and-freedom-of-choice)
   - 4.3. [What Privacy Really Means](#43-what-privacy-really-means)
5. [Data as Power](#5-data-as-power)
   - 5.1. [Data as a Toxic Asset](#51-data-as-a-toxic-asset)
   - 5.2. [Lessons from the Industrial Revolution](#52-lessons-from-the-industrial-revolution)
6. [What We Can Do](#6-what-we-can-do)
   - 6.1. [Legislation and Self-Regulation](#61-legislation-and-self-regulation)
   - 6.2. [A Culture Shift](#62-a-culture-shift)
7. [Summary: The Whole Book](#7-summary-the-whole-book)

---

## 1. Introduction

In this final chapter, we step back from the technical details to examine something fundamental: **what are we building, and what are its consequences?**

Throughout this book, we've explored architectures for reliable, scalable, and maintainable systems. But we've left out a crucial question: Is what we're building *good*?

### 1.1. Data is About People

**In plain English:** When we talk about "data," it's easy to think of it as abstract bits and bytes. But many datasets are about *people*â€”their behavior, interests, identities, relationships, health, and finances. Behind every row in a database is a human being.

**In technical terms:** User activity logs, purchase histories, location data, communication metadata, and behavioral profiles all represent aspects of real people's lives. We must treat such data with humanity and respect.

**Why it matters:** Software development increasingly involves ethical choices. Guidelines like the ACM Code of Ethics exist, but they're rarely discussed or enforced in practice. The result is sometimes a cavalier attitude toward privacy and potential harm.

### 1.2. Technology is Not Neutral

> **ðŸ’¡ Insight**
>
> A technology is not good or bad in itselfâ€”what matters is how it is used and how it affects people. A search engine and a weapon share this property: what determines their moral weight is their application and consequences. It is not sufficient for engineers to focus exclusively on technology while ignoring its effects on people.

**What makes something "good" or "bad"?**

Unlike technical concepts with precise definitions, ethics requires interpretation and judgment. Ethics is not a checklist to confirm complianceâ€”it's a participatory, iterative process of reflection, in dialog with the people affected, with accountability for results.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETHICS IS NOT A CHECKLIST                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   âŒ NOT ETHICS:                       âœ… ETHICS:                         â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                        â”€â”€â”€â”€â”€â”€â”€â”€                           â”‚
â”‚   â–¡ Privacy policy exists              â€¢ Ongoing reflection              â”‚
â”‚   â–¡ GDPR checkbox checked              â€¢ Dialog with affected people     â”‚
â”‚   â–¡ Legal reviewed it                  â€¢ Accountability for outcomes     â”‚
â”‚   â–¡ "Not my department"                â€¢ Considering unintended effects  â”‚
â”‚                                        â€¢ Questioning assumptions         â”‚
â”‚                                        â€¢ Iterating when harm is found    â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. Predictive Analytics

Predictive analytics is a major reason for excitement about "big data" and AI. But there's a critical difference between predicting weather and predicting whether a person is likely to reoffend, default on a loan, or make expensive insurance claims.

### 2.1. Algorithmic Prison

**In plain English:** Imagine being repeatedly rejectedâ€”for jobs, apartments, loans, insurance, air travelâ€”without knowing why, and with no way to appeal. That's what happens when algorithms label someone as "risky."

**The problem:**

| Traditional Justice System | Algorithmic Decision-Making |
|---------------------------|----------------------------|
| Presumption of innocence | Presumption of risk |
| Proof required | Pattern matching |
| Right to appeal | Often no recourse |
| Human accountability | "The algorithm decided" |

Someone who has been (accurately or falsely) labeled as risky by algorithms may face systematic exclusion from key aspects of society. This constraint on freedom has been called **"algorithmic prison"**â€”sentenced without trial.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE ALGORITHMIC PRISON                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   Person applies for:                                                     â”‚
â”‚                                                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚   â”‚   Job   â”‚    â”‚  Loan   â”‚    â”‚ Housing â”‚    â”‚Insuranceâ”‚              â”‚
â”‚   â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜              â”‚
â”‚        â”‚              â”‚              â”‚              â”‚                    â”‚
â”‚        â–¼              â–¼              â–¼              â–¼                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚   â”‚              RISK SCORING ALGORITHM                      â”‚           â”‚
â”‚   â”‚                                                          â”‚           â”‚
â”‚   â”‚  Input: Name, address, browsing history, social         â”‚           â”‚
â”‚   â”‚         graph, purchase patterns, location data...      â”‚           â”‚
â”‚   â”‚                                                          â”‚           â”‚
â”‚   â”‚  Output: "HIGH RISK" (reason: unknown/unexplainable)    â”‚           â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                               â”‚                                          â”‚
â”‚                               â–¼                                          â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”‚
â”‚                        â”‚  DENIED   â”‚  â† No explanation                   â”‚
â”‚                        â”‚  DENIED   â”‚  â† No appeal process                â”‚
â”‚                        â”‚  DENIED   â”‚  â† No proof of guilt                â”‚
â”‚                        â”‚  DENIED   â”‚  â† Sentence: indefinite             â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2. Bias and Discrimination

**In plain English:** If you train an AI on biased historical data, it will learn and amplify that bias. It's like asking someone who grew up in a racist household to be an impartial judgeâ€”their prejudices are baked in.

**The laundering problem:**

There's hope that data-driven decisions might be more fair than subjective human judgments. But algorithms learn patterns from historical dataâ€”and if that data contains bias, the algorithm will amplify it.

> **ðŸ’¡ Insight**
>
> "Machine learning is like money laundering for bias." â€” Maciej CegÅ‚owski
>
> This satirizes the belief that an algorithm could somehow take biased data as input and produce fair output. If the past is discriminatory, predictive analytics codify and amplify that discrimination.

**Proxy discrimination:**

Anti-discrimination laws prohibit treating people differently based on protected traits (race, gender, age, disability). But what if other features correlate with protected traits?

| Seemingly Neutral Data | What It Can Reveal |
|-----------------------|-------------------|
| Postal/ZIP code | Race (in segregated neighborhoods) |
| First name | Gender, ethnicity |
| IP address | Location â†’ race, income |
| Purchasing patterns | Religion, health conditions |
| Browser history | Sexual orientation, beliefs |

### 2.3. Responsibility and Accountability

**Who is responsible when algorithms fail?**

- If a human makes a mistake, they can be held accountable
- If a self-driving car causes an accident, who is responsible?
- If a credit algorithm discriminates, is there recourse?
- If your ML system faces judicial review, can you explain how it decided?

**Credit scores vs. predictive analytics:**

| Traditional Credit Score | ML-Based Scoring |
|-------------------------|------------------|
| Based on *your* borrowing history | Based on *people like you* |
| Errors can be corrected | Errors nearly impossible to identify |
| "How did you behave?" | "Who is similar to you?" |
| Specific, auditable factors | Opaque, unexplainable patterns |

> **ðŸ’¡ Insight**
>
> Much data is statisticalâ€”even if the probability distribution is correct overall, individual cases may be wrong. If average life expectancy is 80 years, that doesn't mean you'll die on your 80th birthday. Similarly, a prediction system's output may be correct on average but wrong for *you* specifically.

---

## 3. Feedback Loops

### 3.1. Self-Reinforcing Problems

Predictive analytics create **feedback loops** where predictions influence outcomes, which then reinforce the predictions.

**Example: The credit score trap**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SELF-REINFORCING FEEDBACK LOOP                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚   â”‚ Good worker,    â”‚                                                    â”‚
â”‚   â”‚ good credit     â”‚                                                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚            â”‚                                                              â”‚
â”‚            â–¼  Unexpected misfortune (medical emergency, job loss)        â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚   â”‚ Missed payments â”‚                                                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚            â”‚                                                              â”‚
â”‚            â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚   â”‚ Credit score    â”‚                                                    â”‚
â”‚   â”‚ drops           â”‚â—€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚                    â”‚
â”‚            â”‚                                         â”‚                    â”‚
â”‚            â–¼                                         â”‚                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚                    â”‚
â”‚   â”‚ Harder to find  â”‚                               â”‚ Reinforces         â”‚
â”‚   â”‚ employment      â”‚                               â”‚                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                               â”‚                    â”‚
â”‚            â”‚                                         â”‚                    â”‚
â”‚            â–¼                                         â”‚                    â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                               â”‚                    â”‚
â”‚   â”‚ Prolonged       â”‚                               â”‚                    â”‚
â”‚   â”‚ joblessness     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                                    â”‚
â”‚            â”‚                                                              â”‚
â”‚            â–¼                                                              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                                    â”‚
â”‚   â”‚ Deeper poverty  â”‚  â† Downward spiral due to poisonous                â”‚
â”‚   â”‚ & worse scores  â”‚    assumptions, hidden behind                      â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    mathematical rigor                              â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Example: Algorithmic price collusion**

When gas stations in Germany introduced algorithmic pricing, economists found that competition *decreased* and consumer prices *increased*â€”because the algorithms learned to collude without any explicit agreement.

### 3.2. Systems Thinking

We can't always predict feedback loops. But we can try by using **systems thinking**â€”analyzing not just the computerized parts, but the entire system including people interacting with it.

**Key questions:**
- Does the system reinforce existing differences (making the rich richer, poor poorer)?
- Or does it combat injustice?
- What are the unintended consequences?

---

## 4. Privacy and Surveillance

### 4.1. The Surveillance Thought Experiment

**Try replacing "data" with "surveillance":**

> "In our **surveillance**-driven organization, we collect real-time **surveillance** streams and store them in our **surveillance** warehouse. Our **surveillance** scientists use advanced analytics and **surveillance** processing to derive new insights."

Does that still sound good? The point is stark but necessary for this book: *Designing Surveillance-Intensive Applications*.

> **ðŸ’¡ Insight**
>
> In our attempt to make software "eat the world," we have built the greatest mass surveillance infrastructure the world has ever seen. We are rapidly approaching a world where every inhabited space contains at least one internet-connected microphoneâ€”smartphones, smart TVs, voice assistants, baby monitors, even children's toys with cloud-based speech recognition.

**Historical perspective:**

Even the most totalitarian regimes could only *dream* of:
- Putting a microphone in every room
- Forcing everyone to carry a device tracking their location
- Recording all communications and purchases

Yet we now *voluntarily* accept this. The difference? The data is collected by corporations for services, not governments for control. But the capability is the same.

### 4.2. Consent and Freedom of Choice

**"Users agreed to the privacy policy"**

This argument has several problems:

| Claim | Reality |
|-------|---------|
| "Users consented" | Privacy policies obscure rather than illuminate |
| "It's a fair exchange" | No negotiation; terms are take-it-or-leave-it |
| "They can opt out" | Opting out of essential services isn't realistic |
| "Data is only about them" | Your data reveals things about others too |

**The GDPR standard for consent:**

The EU's General Data Protection Regulation requires that consent be:
- **Freely given** â€” not coerced
- **Specific** â€” for defined purposes
- **Informed** â€” clearly explained
- **Unambiguous** â€” no pre-ticked boxes or silence

If withdrawing consent causes detriment, it wasn't "freely given."

**Network effects trap:**

For services that are "regarded by most people as essential for basic social participation," opting out has real social costs. Choosing not to use a dominant social network means missing opportunitiesâ€”professional, social, informational.

### 4.3. What Privacy Really Means

**Privacy is NOT about keeping secrets.** It's about having the **freedom to choose** what to reveal, to whom, for what purpose.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    WHAT PRIVACY ACTUALLY MEANS                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   âŒ MISCONCEPTION:           âœ… REALITY:                                 â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                 â”‚
â”‚   "Privacy means              "Privacy means having the                   â”‚
â”‚    keeping everything          CHOICE about what to reveal,               â”‚
â”‚    secret"                     to whom, and for what purpose"             â”‚
â”‚                                                                           â”‚
â”‚   EXAMPLE:                                                                â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€                                                                â”‚
â”‚   Someone with a rare disease might WANT to share their                  â”‚
â”‚   medical data with researchers who could develop treatments.            â”‚
â”‚                                                                           â”‚
â”‚   But they would NOT want that data to affect their:                     â”‚
â”‚   â€¢ Insurance coverage                                                    â”‚
â”‚   â€¢ Employment opportunities                                              â”‚
â”‚   â€¢ Social relationships                                                  â”‚
â”‚                                                                           â”‚
â”‚   Privacy = THEY decide, not a corporation's profit model                â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**When surveillance collects data, privacy rights are transferred:**

Companies say "trust us to do the right thing"â€”meaning the right to decide what to reveal and what to keep secret shifts from the individual to the company. The company then exercises that privacy right to maximize profit, not to serve the individual.

---

## 5. Data as Power

### 5.1. Data as a Toxic Asset

Data is sometimes called "exhaust"â€”worthless waste to be recycled. But this view is backwards. From an economic perspective, if targeted advertising pays for a service, then user activity that generates behavioral data is a form of **labor**.

**Data is valuable:**
- Data brokers buy, aggregate, analyze, and resell personal information
- Startups are valued by "eyeballs"â€”i.e., surveillance capabilities
- When companies go bankrupt, user data is sold as an asset

**Data is dangerous:**

| Risk | Consequence |
|------|-------------|
| Data breaches | Intimate details exposed to criminals |
| Government access | Secret deals, legal compulsion, or theft |
| Company acquisition | New owners may not share your values |
| Regime change | Data collected today may be used by future authoritarian governments |

> **ðŸ’¡ Insight**
>
> Data has been called a "toxic asset" or "hazardous material." Maybe data is not the new gold, nor the new oil, but rather **the new uranium**â€”incredibly powerful and amazingly dangerous. "It is poor civic hygiene to install technologies that could someday facilitate a police state."

**"Knowledge is power":**

To scrutinize others while avoiding scrutiny oneself is one of the most important forms of power. This is why totalitarian governments want surveillance. Although tech companies don't overtly seek political power, the data they've accumulated gives them immense power over our livesâ€”largely surreptitious, outside public oversight.

### 5.2. Lessons from the Industrial Revolution

**In plain English:** The Industrial Revolution brought economic growth and improved living standardsâ€”but also child labor, dangerous working conditions, and terrible pollution. It took decades to establish safeguards. We're in a similar transition with data.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              INDUSTRIAL REVOLUTION vs. INFORMATION AGE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   INDUSTRIAL REVOLUTION               INFORMATION AGE                     â”‚
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€              â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                     â”‚
â”‚                                                                           â”‚
â”‚   Problem: Pollution                  Problem: Data collection            â”‚
â”‚   â€¢ Air (smoke, chemicals)            â€¢ Behavioral surveillance           â”‚
â”‚   â€¢ Water (industrial waste)          â€¢ Location tracking                 â”‚
â”‚   â€¢ Child labor                       â€¢ Predictive profiling              â”‚
â”‚   â€¢ Dangerous workplaces              â€¢ Algorithmic discrimination        â”‚
â”‚                                                                           â”‚
â”‚   Solution: Regulation                Solution: ???                        â”‚
â”‚   â€¢ Environmental protection          â€¢ GDPR (partial)                    â”‚
â”‚   â€¢ Safety protocols                  â€¢ Industry self-regulation          â”‚
â”‚   â€¢ Child labor laws                  â€¢ Engineering ethics                â”‚
â”‚   â€¢ Health inspections                â€¢ Culture shift needed              â”‚
â”‚                                                                           â”‚
â”‚   "Data is the pollution problem of the information age,                 â”‚
â”‚    and protecting privacy is the environmental challenge."               â”‚
â”‚                                       â€” Bruce Schneier                    â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> **ðŸ’¡ Insight**
>
> Just as we look back at the early Industrial Revolution and wonder how our ancestors could have ignored pollution in their rush to build an industrial world, our grandchildren will look back at us during these early decades of the information age and judge us on how we addressed the challenge of data collection and misuse. We should try to make them proud.

---

## 6. What We Can Do

### 6.1. Legislation and Self-Regulation

**GDPR principles:**

The European GDPR states that personal data must be:
- Collected for **specified, explicit, and legitimate purposes**
- **Not processed** in ways incompatible with those purposes
- **Adequate, relevant, and limited** to what is necessary

**The tension:**

This principle of **data minimization** directly conflicts with the philosophy of Big Data, which maximizes data collection, combines datasets, and explores for unexpected insights. "Exploration" means using data for *unforeseen* purposesâ€”the opposite of "specified and explicit."

**Reality check:**
- GDPR has had some effect on online advertising
- But enforcement has been weak
- Little cultural change in the broader tech industry

### 6.2. A Culture Shift

Fundamentally, we need a **culture shift** in tech:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE CULTURE SHIFT WE NEED                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                           â”‚
â”‚   STOP:                                START:                             â”‚
â”‚   â”€â”€â”€â”€â”€                                â”€â”€â”€â”€â”€â”€                             â”‚
â”‚   â€¢ Viewing users as metrics           â€¢ Remembering users are humans     â”‚
â”‚   â€¢ Maximizing data collection         â€¢ Minimizing what we collect       â”‚
â”‚   â€¢ Keeping users in the dark          â€¢ Educating users about data use   â”‚
â”‚   â€¢ Retaining data forever             â€¢ Purging when no longer needed    â”‚
â”‚   â€¢ Treating privacy as obstacle       â€¢ Treating privacy as fundamental  â”‚
â”‚   â€¢ "Not my department"                â€¢ Taking responsibility            â”‚
â”‚                                                                           â”‚
â”‚   PRINCIPLE: Data you don't have is data that can't be:                  â”‚
â”‚   â€¢ Leaked                                                                â”‚
â”‚   â€¢ Stolen                                                                â”‚
â”‚   â€¢ Compelled by governments to be handed over                           â”‚
â”‚                                                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**As engineers, if we don't consider the societal impact of our work, we're not doing our job.**

Individual rights over personal data are like a national parkâ€”if we don't explicitly protect and care for them, they will be destroyed. It will be the tragedy of the commons, and we will all be worse off.

Ubiquitous surveillance is not inevitable. We are still able to stop it.

---

## 7. Summary: The Whole Book

This brings us to the end of the book. Let's recap the journey:

### Part I: Foundations of Data Systems

**Chapter 1 â€” Trade-offs:** Analytical vs. operational systems, cloud vs. self-hosting, distributed vs. single-node, balancing business and user needs.

**Chapter 2 â€” Nonfunctional Requirements:** Performance, reliability, scalability, and maintainability.

**Chapter 3 â€” Data Models:** Relational, document, and graph models; event sourcing; DataFrames. Query languages: SQL, Cypher, SPARQL, Datalog, GraphQL.

**Chapter 4 â€” Storage and Retrieval:** LSM-trees and B-trees for OLTP, column-oriented storage for analytics, full-text and vector search for information retrieval.

**Chapter 5 â€” Encoding and Evolution:** Data serialization formats, schema evolution, and data flow via databases, services, workflows, and events.

### Part II: Distributed Data

**Chapter 6 â€” Replication:** Single-leader, multi-leader, and leaderless replication; consistency models; sync engines for offline operation.

**Chapter 7 â€” Sharding:** Partitioning strategies, rebalancing, request routing, secondary indexes.

**Chapter 8 â€” Transactions:** Durability, isolation levels (read committed, snapshot, serializable), distributed transactions.

**Chapter 9 â€” Distributed Systems Fundamentals:** Network faults, clock errors, process pauses, and why even simple things like locks are hard.

**Chapter 10 â€” Consistency and Consensus:** Consensus algorithms and linearizability.

### Part III: Derived Data

**Chapter 11 â€” Batch Processing:** Unix tools to MapReduce to modern dataflow engines; distributed filesystems and object stores.

**Chapter 12 â€” Stream Processing:** Message brokers, change data capture, fault tolerance, streaming joins.

**Chapter 13 â€” Streaming Philosophy:** Integrating disparate systems, evolving systems, scaling applications.

**Chapter 14 â€” Doing the Right Thing:** Data can do good, but also harmâ€”discrimination, surveillance, exploitation. We carry responsibility.

---

### Final Thoughts

> **ðŸ’¡ Insight**
>
> Data can be used to do good, but it can also do significant harm: making decisions that seriously affect people's lives and are difficult to appeal; leading to discrimination and exploitation; normalizing surveillance; exposing intimate information. We run the risk of data breaches, and well-intentioned uses may have unintended consequences.

As software and data have such a large impact on the world, we as engineers must remember that we carry a responsibility to work toward the kind of world we want to live in: **a world that treats people with humanity and respect.**

Let's work together toward that goal.

---

**Previous:** [Chapter 13](chapter13-streaming-philosophy.md) | **Next:** [Glossary](#)

---
sidebar_position: 1
title: "Chapter 11. Batch Processing"
description: "Learn about batch processing systems, MapReduce, distributed filesystems, and how to process massive datasets efficiently"
---

import { Box, Arrow, Row, Column, Group, DiagramContainer, ProcessFlow, TreeDiagram, CardGrid, ConnectionDiagram, StackDiagram, ComparisonTable, StateFlow, colors } from '@site/src/components/diagrams';

# Chapter 11. Batch Processing

> A system cannot be successful if it is too strongly influenced by a single person. Once the initial design is complete and fairly robust, the real test begins as people with many different viewpoints undertake their own experiments.
>
> _Donald Knuth_

## Table of Contents

1. [Introduction](#1-introduction)
   - 1.1. [Online vs. Batch vs. Stream](#11-online-vs-batch-vs-stream)
   - 1.2. [Why Batch Processing Matters](#12-why-batch-processing-matters)
2. [Batch Processing with Unix Tools](#2-batch-processing-with-unix-tools)
   - 2.1. [Simple Log Analysis](#21-simple-log-analysis)
   - 2.2. [Chain of Commands vs. Custom Program](#22-chain-of-commands-vs-custom-program)
   - 2.3. [Sorting vs. In-Memory Aggregation](#23-sorting-vs-in-memory-aggregation)
3. [Batch Processing in Distributed Systems](#3-batch-processing-in-distributed-systems)
   - 3.1. [Distributed Filesystems](#31-distributed-filesystems)
   - 3.2. [Object Stores](#32-object-stores)
   - 3.3. [Distributed Job Orchestration](#33-distributed-job-orchestration)
   - 3.4. [Resource Allocation](#34-resource-allocation)
   - 3.5. [Scheduling Workflows](#35-scheduling-workflows)
   - 3.6. [Handling Faults](#36-handling-faults)
4. [Batch Processing Models](#4-batch-processing-models)
   - 4.1. [MapReduce](#41-mapreduce)
   - 4.2. [Dataflow Engines](#42-dataflow-engines)
   - 4.3. [Shuffling Data](#43-shuffling-data)
   - 4.4. [JOIN and GROUP BY](#44-join-and-group-by)
5. [Query Languages and DataFrames](#5-query-languages-and-dataframes)
   - 5.1. [SQL for Batch Processing](#51-sql-for-batch-processing)
   - 5.2. [DataFrames](#52-dataframes)
   - 5.3. [Batch Processing and Data Warehouses Converge](#53-batch-processing-and-data-warehouses-converge)
6. [Batch Use Cases](#6-batch-use-cases)
   - 6.1. [Extract-Transform-Load (ETL)](#61-extract-transform-load-etl)
   - 6.2. [Analytics](#62-analytics)
   - 6.3. [Machine Learning](#63-machine-learning)
   - 6.4. [Bulk Data Imports](#64-bulk-data-imports)
7. [Summary](#7-summary)

---

## 1. Introduction

**In plain English:**
- Think of batch processing like doing laundry
- Instead of washing each shirt individually (exhausting!)
- You wait until you have a full load
- Then process everything at once
- Same with data‚Äîcollect it, process together

**In technical terms:**
- Batch processing takes a set of input data (read-only)
- Produces output data (generated from scratch every run)
- Unlike online transactions, batch jobs don't mutate data
- They derive new outputs from existing inputs

**Why it matters:**
- Bug in the output? Roll back code, rerun the job
- This "human fault tolerance" enables faster development
- Mistakes aren't irreversible

### 1.1. Online vs. Batch vs. Stream

<DiagramContainer title="THREE STYLES OF DATA PROCESSING">
  <Row gap={40} align="start">
    <Column gap={15}>
      <Box color={colors.blue} label="ONLINE SYSTEMS" padding={15} />
      <Column gap={10}>
        <div style={{fontSize: '14px', textAlign: 'center'}}>Request ‚Üí Response</div>
        <div style={{fontSize: '13px', textAlign: 'center'}}>User clicks "Buy Now"</div>
        <Arrow direction="down" size={30} />
        <div style={{fontSize: '13px', textAlign: 'center'}}>Response in milliseconds</div>
        <Box color={colors.blue} label="Primary Metric: Latency" padding={10} />
        <div style={{fontSize: '12px', marginTop: '10px'}}>
          <strong>Examples:</strong>
          <br/>- Web servers
          <br/>- Databases
          <br/>- APIs
        </div>
      </Column>
    </Column>

    <Column gap={15}>
      <Box color={colors.green} label="BATCH SYSTEMS" padding={15} />
      <Column gap={10}>
        <div style={{fontSize: '14px', textAlign: 'center'}}>Input ‚Üí Output</div>
        <div style={{fontSize: '13px', textAlign: 'center'}}>Job processes yesterday's logs</div>
        <Arrow direction="down" size={30} />
        <div style={{fontSize: '13px', textAlign: 'center'}}>Results in minutes/hours</div>
        <Box color={colors.green} label="Primary Metric: Throughput" padding={10} />
        <div style={{fontSize: '12px', marginTop: '10px'}}>
          <strong>Examples:</strong>
          <br/>- Log analysis
          <br/>- ML training
          <br/>- ETL pipelines
        </div>
      </Column>
    </Column>

    <Column gap={15}>
      <Box color={colors.purple} label="STREAM SYSTEMS" padding={15} />
      <Column gap={10}>
        <div style={{fontSize: '14px', textAlign: 'center'}}>Event ‚Üí Action</div>
        <div style={{fontSize: '13px', textAlign: 'center'}}>Message arrives in queue</div>
        <Arrow direction="down" size={30} />
        <div style={{fontSize: '13px', textAlign: 'center'}}>Processed in seconds</div>
        <Box color={colors.purple} label="Primary Metric: Freshness" padding={10} />
        <div style={{fontSize: '12px', marginTop: '10px'}}>
          <strong>Examples:</strong>
          <br/>- Fraud detection
          <br/>- Live dashboards
          <br/>- CDC replication
        </div>
      </Column>
    </Column>
  </Row>
</DiagramContainer>

| System Type | Response Time | Input | Examples |
|-------------|---------------|-------|----------|
| **Online** | Milliseconds | Individual requests | Web servers, APIs, databases |
| **Batch** | Minutes to days | Bounded datasets | ETL, ML training, analytics |
| **Stream** | Seconds | Unbounded events | Real-time dashboards, CDC |

### 1.2. Why Batch Processing Matters

**Batch processing offers unique advantages:**
- A fundamental building block for reliable systems
- Enables patterns impossible in online systems

> **üí° Insight**
>
> The key property of batch processing is **immutability**: inputs are never modified. This seemingly simple constraint has profound implications:
>
> - Something goes wrong? Rerun the job to produce correct output
> - Same input files can be used by multiple jobs for different purposes
> - Compare outputs across job runs to detect anomalies

**Benefits of batch processing:**

1. **Human fault tolerance** ‚Äî If you deploy buggy code that produces wrong output, just roll back and rerun. Compare this to a database where bad writes corrupt data permanently.

2. **Reproducibility** ‚Äî The same input always produces the same output, making debugging straightforward.

3. **Parallel processing** ‚Äî Different parts of the input can be processed independently, enabling horizontal scaling.

4. **Cost efficiency** ‚Äî Jobs can run during off-peak hours, use spot instances, and process massive amounts of data economically.

---

## 2. Batch Processing with Unix Tools

**Before distributed systems, let's understand batch processing with Unix:**
- Tools you already have
- Same patterns scale to massive systems

### 2.1. Simple Log Analysis

Say you have a web server log file and want to find the five most popular pages:

```
216.58.210.78 - - [27/Jun/2025:17:55:11 +0000] "GET /css/typography.css HTTP/1.1"
200 3377 "https://martin.kleppmann.com/" "Mozilla/5.0 ..."
```

You can analyze it with a chain of Unix commands:

```bash
cat /var/log/nginx/access.log |   # 1. Read the log file
  awk '{print $7}' |               # 2. Extract the URL (7th field)
  sort             |               # 3. Sort URLs alphabetically
  uniq -c          |               # 4. Count consecutive duplicates
  sort -r -n       |               # 5. Sort by count (descending)
  head -n 5                        # 6. Take top 5
```

**Output:**
```
4189 /favicon.ico
3631 /2016/02/08/how-to-do-distributed-locking.html
2124 /2020/11/18/distributed-systems-and-elliptic-curves.html
1369 /
 915 /css/typography.css
```

<DiagramContainer title="UNIX PIPELINE DATA FLOW">
  <Column gap={20}>
    <div style={{textAlign: 'center', fontSize: '14px', fontWeight: 'bold'}}>access.log</div>
    <Arrow direction="down" />
    <ProcessFlow
      steps={[
        {label: "cat", description: "Full lines from file"},
        {label: "awk", description: "URLs only extracted"},
        {label: "sort", description: "Sorted A-Z"},
        {label: "uniq -c", description: "Counted per URL"},
        {label: "sort", description: "By count descending"},
        {label: "head", description: "Top 5 URLs"}
      ]}
      color={colors.blue}
    />
  </Column>
</DiagramContainer>

> **üí° Insight**
>
> This pipeline processes gigabytes in seconds because each tool does one thing well and data flows between them efficiently. The key pattern is **sort ‚Üí group ‚Üí aggregate**, which is exactly what distributed batch systems like MapReduce use at massive scale.

### 2.2. Chain of Commands vs. Custom Program

The same analysis in Python:

```python
from collections import defaultdict

counts = defaultdict(int)  # Counter for each URL

with open('/var/log/nginx/access.log', 'r') as file:
    for line in file:
        url = line.split()[6]  # Extract URL (7th field, 0-indexed)
        counts[url] += 1       # Increment counter

# Sort by count descending, take top 5
top5 = sorted(((count, url) for url, count in counts.items()),
              reverse=True)[:5]

for count, url in top5:
    print(f"{count} {url}")
```

Both approaches work, but the execution models differ fundamentally:

| Aspect | Unix Pipeline | Python Script |
|--------|---------------|---------------|
| **Aggregation** | Sort then count adjacent | Hash table in memory |
| **Memory** | Streaming (minimal) | Proportional to unique URLs |
| **Large data** | Spills to disk automatically | May run out of memory |

### 2.3. Sorting vs. In-Memory Aggregation

**In plain English:** Imagine counting votes. You could either keep a tally sheet (hash table) where you update counts as you go, or you could sort all the ballots by candidate name first, then just count how many are in each pile.

**When to use which:**

- **Hash table (in-memory)**: Fast when all unique keys fit in memory. If you have 1 million log entries but only 10,000 unique URLs, a hash table works great.

- **Sorting**: Better when data exceeds memory. The `sort` utility automatically spills to disk and parallelizes across CPU cores. Mergesort has sequential access patterns that perform well on disk.

> **üí° Insight**
>
> The Unix `sort` command is deceptively powerful‚Äîit automatically handles larger-than-memory datasets by spilling to disk and parallelizes across CPU cores. This is the same principle that distributed batch systems use: sort, merge, and scan.

**Limitation:** Unix tools run on a single machine. When datasets exceed local disk capacity, we need distributed batch processing frameworks.

---

## 3. Batch Processing in Distributed Systems

**A distributed batch framework is a distributed operating system:**
- Just as your laptop has storage, scheduler, programs connected by pipes
- Distributed frameworks have the same components‚Äîat scale

<DiagramContainer title="SINGLE MACHINE vs. DISTRIBUTED BATCH SYSTEM">
  <Row gap={60} align="start">
    <Column gap={15}>
      <Box color={colors.blue} label="SINGLE MACHINE" padding={10} />
      <Column gap={20}>
        <Box color={colors.gray} label="Local Disk (ext4, XFS)" padding={10} />
        <Box color={colors.gray} label="OS Scheduler" padding={10} />
        <Box color={colors.gray} label="Unix Pipes" padding={10} />
        <Box color={colors.gray} label="Processes (awk, sort)" padding={10} />
      </Column>
    </Column>

    <div style={{alignSelf: 'center', fontSize: '24px'}}>‚Üí</div>

    <Column gap={15}>
      <Box color={colors.green} label="DISTRIBUTED SYSTEM" padding={10} />
      <Column gap={20}>
        <Box color={colors.gray} label="Distributed FS (HDFS, S3)" padding={10} />
        <Box color={colors.gray} label="Job Orchestrator (YARN, K8s)" padding={10} />
        <Box color={colors.gray} label="Shuffle/Network (data transfer)" padding={10} />
        <Box color={colors.gray} label="Tasks (mappers, reducers)" padding={10} />
      </Column>
    </Column>
  </Row>
</DiagramContainer>

### 3.1. Distributed Filesystems

**In plain English:**
- Think of a distributed filesystem like a library with multiple branches
- Books (data blocks) are stored across different branches (machines)
- There's a central catalog (metadata service) that knows where everything is
- If one branch burns down, copies exist at other branches

**Key components:**

| Component | Local Filesystem | Distributed Filesystem |
|-----------|------------------|------------------------|
| **Block size** | 4 KB (ext4) | 128 MB (HDFS) or 4 MB (S3) |
| **Data nodes** | Single disk | Many machines |
| **Metadata** | Inodes on disk | NameNode / metadata service |
| **Redundancy** | RAID | Replication or erasure coding |
| **Access** | VFS API | DFS protocol (HDFS, S3 API) |

**How it works:**

1. Files are split into large blocks (128 MB in HDFS)
2. Each block is replicated across multiple machines (typically 3)
3. A metadata service tracks which machines store which blocks
4. Clients read blocks from any replica; writes go to all replicas

<DiagramContainer title="DISTRIBUTED FILESYSTEM ARCHITECTURE">
  <Column gap={30}>
    <Box color={colors.blue} label="NameNode / Metadata Service (file‚Üíblocks)" padding={15} />
    <Arrow direction="down" />
    <Row gap={30}>
      <Column gap={10}>
        <Box color={colors.green} label="DataNode 1" padding={10} />
        <div style={{fontSize: '12px', textAlign: 'center'}}>
          Block A<br/>Block B<br/>Block D
        </div>
      </Column>
      <Column gap={10}>
        <Box color={colors.green} label="DataNode 2" padding={10} />
        <div style={{fontSize: '12px', textAlign: 'center'}}>
          Block A<br/>Block C<br/>Block D
        </div>
      </Column>
      <Column gap={10}>
        <Box color={colors.green} label="DataNode 3" padding={10} />
        <div style={{fontSize: '12px', textAlign: 'center'}}>
          Block B<br/>Block C<br/>Block A
        </div>
      </Column>
    </Row>
    <div style={{textAlign: 'center', fontSize: '13px', marginTop: '15px'}}>
      File "logs.parquet" = [Block A, Block B, Block C, Block D]<br/>
      Each block replicated 3x across different nodes
    </div>
  </Column>
</DiagramContainer>

> **üí° Insight**
>
> DFS blocks are much larger than local filesystem blocks (128 MB vs 4 KB) because:
>
> - Overhead of tracking each block scales with the number of blocks
> - At petabyte scale, millions of 4 KB blocks would overwhelm metadata service
> - Large blocks amortize network overhead
> - More efficient to stream 128 MB than make 32,000 separate 4 KB requests

### 3.2. Object Stores

**Object stores have become the dominant storage layer:**
- S3, GCS, Azure Blob
- Replacing HDFS in many deployments
- Simpler operations model

**Key differences from distributed filesystems:**

| Feature | Distributed FS (HDFS) | Object Store (S3) |
|---------|----------------------|-------------------|
| **Operations** | Open, seek, read, write, close | GET, PUT (whole object) |
| **Mutability** | Files can be appended | Objects are immutable |
| **Directories** | True directories | Key prefixes (simulated) |
| **Renames** | Atomic | Copy + delete (non-atomic) |
| **Compute locality** | Tasks run on data nodes | Storage/compute separated |
| **Cost model** | Capacity-based | Request + capacity |

**Object URL structure:**
```
s3://my-data-bucket/2025/06/27/events.parquet
     ‚îî‚îÄ‚îÄ bucket ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ key ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key point:**
- The slashes in the key are just conventions‚Äîno real directories
- Listing objects with prefix `2025/06/` returns all matching keys

### 3.3. Distributed Job Orchestration

**In plain English:**
- An orchestrator is like a construction site foreman
- When you want to build something (run a job):
  - The foreman figures out which workers are available
  - Assigns tasks
  - Monitors progress
  - Handles problems when workers get sick or equipment breaks

**Components of job orchestration:**

<DiagramContainer title="JOB ORCHESTRATION COMPONENTS">
  <Column gap={20}>
    <Box color={colors.blue} label="SCHEDULER" padding={15}>
      <div style={{fontSize: '12px', marginTop: '8px'}}>
        ‚Ä¢ Receives job requests<br/>
        ‚Ä¢ Decides which tasks run on which nodes<br/>
        ‚Ä¢ Balances fairness vs. efficiency
      </div>
    </Box>
    <Arrow direction="down" />
    <Box color={colors.green} label="RESOURCE MANAGER" padding={15}>
      <div style={{fontSize: '12px', marginTop: '8px'}}>
        ‚Ä¢ Tracks all nodes and their resources (CPU, GPU, memory)<br/>
        ‚Ä¢ Maintains global cluster state in ZooKeeper/etcd<br/>
        ‚Ä¢ Knows what's running where
      </div>
    </Box>
    <Arrow direction="down" />
    <Row gap={30}>
      <Column gap={10}>
        <Box color={colors.purple} label="Executor (Node 1)" padding={10} />
        <div style={{fontSize: '11px', textAlign: 'center'}}>Task A<br/>Task D</div>
      </Column>
      <Column gap={10}>
        <Box color={colors.purple} label="Executor (Node 2)" padding={10} />
        <div style={{fontSize: '11px', textAlign: 'center'}}>Task B<br/>Task E</div>
      </Column>
      <Column gap={10}>
        <Box color={colors.purple} label="Executor (Node 3)" padding={10} />
        <div style={{fontSize: '11px', textAlign: 'center'}}>Task C<br/>Task F</div>
      </Column>
    </Row>
    <div style={{fontSize: '12px', textAlign: 'center', marginTop: '10px'}}>
      Executors: Run tasks, send heartbeats, report status<br/>
      Use cgroups for resource isolation between tasks
    </div>
  </Column>
</DiagramContainer>

**Job request metadata includes:**
- Number of tasks to execute
- Resources per task (CPU, memory, disk, GPU)
- Job identifier and access credentials
- Input/output data locations
- Executable code location

### 3.4. Resource Allocation

**The scheduling problem:**
- Scheduling is **NP-hard**‚Äîoptimal allocation is computationally infeasible
- Real schedulers use heuristics instead

**Consider this scenario:**

> A cluster has 160 CPU cores. Two jobs arrive, each requesting 100 cores. What should the scheduler do?

**Options and trade-offs:**

| Strategy | Behavior | Trade-off |
|----------|----------|-----------|
| **Fair share** | Run 80 tasks from each job | Neither finishes as fast as possible |
| **Gang scheduling** | Wait for all 100 cores, run one job | Nodes sit idle while waiting |
| **FIFO** | First job gets everything | Second job may starve |
| **Preemption** | Kill some tasks to make room | Wasted work from killed tasks |

**Common scheduling heuristics:**
- FIFO (first come, first served)
- Dominant Resource Fairness (DRF)
- Priority queues
- Capacity-based scheduling
- Bin-packing algorithms

### 3.5. Scheduling Workflows

**Batch jobs often form workflows (DAGs):**
- **DAG** = Directed Acyclic Graph
- Output of one job feeds into another
- Scheduler ensures jobs run in correct order

<DiagramContainer title="WORKFLOW / DAG EXAMPLE">
  <Column gap={20} align="center">
    <Box color={colors.blue} label="Raw Events (Input)" padding={15} />
    <Arrow direction="down" />
    <Box color={colors.green} label="Clean & Parse" padding={15} />
    <Arrow direction="down" />
    <Row gap={30}>
      <Column gap={10}>
        <Box color={colors.purple} label="Aggregate by Region" padding={12} />
        <Arrow direction="down" />
        <Box color={colors.gray} label="Regional Report" padding={10} />
      </Column>
      <Column gap={10}>
        <Box color={colors.purple} label="Join User Profiles" padding={12} />
        <Arrow direction="down" />
        <Box color={colors.gray} label="Joined Data" padding={10} />
      </Column>
      <Column gap={10}>
        <Box color={colors.purple} label="Feature Engineer" padding={12} />
        <Arrow direction="down" />
        <Box color={colors.gray} label="Training Data" padding={10} />
      </Column>
    </Row>
    <div style={{fontSize: '13px', textAlign: 'center', marginTop: '15px'}}>
      Workflow schedulers: Airflow, Dagster, Prefect<br/>
      Wait for all inputs before running dependent jobs
    </div>
  </Column>
</DiagramContainer>

> **üí° Insight**
>
> There's an important distinction between:
>
> - **Job orchestrators** (YARN, Kubernetes) ‚Äî schedule individual jobs
> - **Workflow orchestrators** (Airflow, Dagster) ‚Äî manage dependencies between jobs
>
> A workflow with 50-100 interconnected jobs is common in data pipelines.

### 3.6. Handling Faults

**Batch jobs run for long periods‚Äîfailures are inevitable:**
- Jobs run minutes to days
- Many parallel tasks
- Multiple failure modes:

- Hardware faults (especially on commodity hardware)
- Network interruptions
- **Preemption** by higher-priority jobs
- Spot instance terminations (to save cost)

**Fault tolerance strategies:**

| System | Intermediate Data | Recovery Method |
|--------|-------------------|-----------------|
| **MapReduce** | Written to DFS | Reread from DFS |
| **Spark** | Kept in memory | Recompute from lineage |
| **Flink** | Periodic checkpoints | Restore from checkpoint |

> **üí° Insight**
>
> Because batch jobs regenerate output from scratch, fault recovery is simpler:
>
> - Just delete partial output and rerun the failed task
> - This wouldn't work if the job had side effects (like sending emails)
> - That's why batch processing emphasizes immutable inputs and pure transformations

---

## 4. Batch Processing Models

### 4.1. MapReduce

**MapReduce mirrors our Unix log analysis example:**
- Same pattern: read ‚Üí extract ‚Üí sort ‚Üí group ‚Üí aggregate

<DiagramContainer title="MAPREDUCE PIPELINE">
  <Column gap={20}>
    <div style={{fontSize: '14px', textAlign: 'center', fontFamily: 'monospace'}}>
      Unix: cat | awk | sort | uniq -c | sort -rn | head
    </div>
    <Arrow direction="down" />
    <div style={{fontSize: '14px', textAlign: 'center'}}>
      MapReduce: Read ‚Üí Map ‚Üí Shuffle ‚Üí Reduce ‚Üí (Second MapReduce job)
    </div>
    <div style={{borderTop: '2px solid #ccc', margin: '15px 0'}} />
    <Column gap={15}>
      <Box color={colors.blue} label="STEP 1: READ" padding={12}>
        <div style={{fontSize: '12px', marginTop: '5px'}}>
          Break input files into records<br/>
          Input: Parquet/Avro files on HDFS or S3
        </div>
      </Box>
      <Box color={colors.green} label="STEP 2: MAP" padding={12}>
        <div style={{fontSize: '12px', marginTop: '5px'}}>
          Extract key-value pairs from each record<br/>
          Example: (URL, 1) for each log line
        </div>
      </Box>
      <Box color={colors.purple} label="STEP 3: SHUFFLE (implicit)" padding={12}>
        <div style={{fontSize: '12px', marginTop: '5px'}}>
          Sort by key, group values with same key<br/>
          All values for "page.html" go to same reducer
        </div>
      </Box>
      <Box color={colors.orange} label="STEP 4: REDUCE" padding={12}>
        <div style={{fontSize: '12px', marginTop: '5px'}}>
          Process grouped values, produce output<br/>
          Example: sum up the 1s ‚Üí ("page.html", 42)
        </div>
      </Box>
    </Column>
  </Column>
</DiagramContainer>

**Mapper and Reducer:**

```python
# Mapper: called for each input record
def mapper(record):
    url = record['request_url']
    yield (url, 1)  # Emit key-value pair

# Reducer: called for each unique key with all its values
def reducer(key, values):
    count = sum(values)  # values is iterator over all 1s
    yield (key, count)
```

> **üí° Insight**
>
> MapReduce's programming model comes from **functional programming**‚Äîspecifically Lisp's `map` and `reduce` (fold) higher-order functions:
>
> - **Map** is embarrassingly parallel (each input processed independently)
> - **Reduce** processes each key independently
> - This makes parallelization trivial

**Why MapReduce is mostly obsolete:**

- Requires writing map/reduce in a general-purpose language
- No job pipelining (must wait for upstream job to finish completely)
- Always sorts between map and reduce (even when unnecessary)
- Replaced by Spark, Flink, and SQL-based systems

### 4.2. Dataflow Engines

**Modern engines like Spark and Flink improve on MapReduce:**
- Flexible operators (not just map/reduce)
- In-memory intermediate data
- Pipelined execution

<DiagramContainer title="MAPREDUCE vs. DATAFLOW ENGINES">
  <Column gap={30}>
    <div>
      <div style={{fontSize: '14px', fontWeight: 'bold', marginBottom: '10px'}}>MapReduce:</div>
      <ProcessFlow
        steps={[
          {label: "Map"},
          {label: "Sort"},
          {label: "Reduce"},
          {label: "Map"},
          {label: "Sort"},
          {label: "Reduce"}
        ]}
        color={colors.blue}
      />
      <div style={{fontSize: '12px', textAlign: 'center', marginTop: '8px'}}>
        ‚Üì DFS (always write) ‚Üì DFS (always write)
      </div>
    </div>
    <div style={{borderTop: '2px solid #ccc'}} />
    <div>
      <div style={{fontSize: '14px', fontWeight: 'bold', marginBottom: '10px'}}>Dataflow Engine (Spark/Flink):</div>
      <ProcessFlow
        steps={[
          {label: "Read"},
          {label: "Filter"},
          {label: "Map"},
          {label: "Shuffle"},
          {label: "Aggregate"}
        ]}
        color={colors.green}
      />
      <div style={{fontSize: '12px', textAlign: 'center', marginTop: '8px'}}>
        ‚Üê In memory (only shuffle to disk if needed) ‚Üí
      </div>
    </div>
  </Column>
</DiagramContainer>

**Advantages of dataflow engines:**

| Feature | MapReduce | Dataflow Engines |
|---------|-----------|------------------|
| **Sorting** | Always between stages | Only when needed |
| **Intermediate data** | Written to DFS | In-memory or local disk |
| **Operator fusion** | Each stage separate | Adjacent ops combined |
| **Pipelining** | Wait for stage completion | Stream between stages |
| **Process reuse** | New JVM per task | Reuse processes |

### 4.3. Shuffling Data

**In plain English:**
- Shuffling is like sorting mail at a post office
- Letters arrive from many mailboxes (mappers)
- Need to organize so all letters for same zip code (key)
- End up in the same bin (reducer)

<DiagramContainer title="SHUFFLE IN MAPREDUCE">
  <Row gap={60} align="center">
    <Column gap={20}>
      <div style={{fontSize: '14px', fontWeight: 'bold', textAlign: 'center'}}>Input Shards</div>
      <Column gap={15}>
        <Group>
          <Box color={colors.blue} label="Shard m1" padding={8} />
          <Arrow direction="down" size={20} />
          <Box color={colors.green} label="Mapper 1" padding={8} />
        </Group>
        <Group>
          <Box color={colors.blue} label="Shard m2" padding={8} />
          <Arrow direction="down" size={20} />
          <Box color={colors.green} label="Mapper 2" padding={8} />
        </Group>
        <Group>
          <Box color={colors.blue} label="Shard m3" padding={8} />
          <Arrow direction="down" size={20} />
          <Box color={colors.green} label="Mapper 3" padding={8} />
        </Group>
      </Column>
    </Column>

    <Column gap={10} align="center">
      <div style={{fontSize: '14px', fontWeight: 'bold'}}>Shuffle</div>
      <div style={{fontSize: '12px', textAlign: 'center'}}>(sort by key hash)</div>
      <div style={{fontSize: '24px'}}>‚Üí</div>
    </Column>

    <Column gap={20}>
      <div style={{fontSize: '14px', fontWeight: 'bold', textAlign: 'center'}}>Output Shards</div>
      <Column gap={15}>
        <Box color={colors.purple} label="Shard r1" padding={8} />
        <Box color={colors.purple} label="Shard r2" padding={8} />
        <Box color={colors.purple} label="Shard r3" padding={8} />
      </Column>
    </Column>
  </Row>
  <div style={{fontSize: '12px', textAlign: 'center', marginTop: '20px'}}>
    Each mapper creates sorted files for each reducer<br/>
    hash(key) determines which reducer gets the key-value pair<br/>
    Reducers merge sorted files from all mappers
  </div>
</DiagramContainer>

**Shuffle process:**

1. Each mapper creates a separate output file for each reducer
2. Key hash determines destination: `hash(key) % num_reducers`
3. Mapper sorts key-value pairs within each file
4. Reducers fetch their files from all mappers
5. Reducers merge-sort the files together
6. Same keys are now adjacent ‚Üí reducer iterates over values

> **üí° Insight**
>
> Despite the name, shuffle produces **sorted** order, not random order:
>
> - Term comes from shuffling cards to redistribute, not randomize
> - Modern systems like BigQuery keep shuffle data in memory
> - Use external shuffle services for resilience

### 4.4. JOIN and GROUP BY

**Shuffling enables distributed joins:**
- Bring matching keys to same node
- Process joined data locally

<DiagramContainer title="SORT-MERGE JOIN">
  <Column gap={20}>
    <Row gap={40} align="start">
      <Column gap={10}>
        <div style={{fontSize: '14px', fontWeight: 'bold'}}>ACTIVITY EVENTS</div>
        <Box color={colors.blue} padding={12}>
          <div style={{fontSize: '12px'}}>
            user_id: 123<br/>
            page: /products<br/>
            timestamp: 10:30<br/>
            <br/>
            user_id: 123<br/>
            page: /checkout<br/>
            timestamp: 10:35<br/>
            <br/>
            user_id: 456<br/>
            page: /home
          </div>
        </Box>
      </Column>
      <Column gap={10}>
        <div style={{fontSize: '14px', fontWeight: 'bold'}}>USER PROFILES</div>
        <Box color={colors.green} padding={12}>
          <div style={{fontSize: '12px'}}>
            user_id: 123<br/>
            birth_date: 1990<br/>
            name: Alice
          </div>
        </Box>
      </Column>
    </Row>

    <div style={{fontSize: '14px', fontWeight: 'bold', textAlign: 'center', borderTop: '2px solid #ccc', borderBottom: '2px solid #ccc', padding: '10px', margin: '10px 0'}}>
      SHUFFLE BY user_id
    </div>

    <Box color={colors.purple} label="REDUCER FOR user_id=123:" padding={12}>
      <div style={{fontSize: '12px', marginTop: '5px'}}>
        1. User profile: {'{'}birth_date: 1990, name: Alice{'}'} ‚Üê arrives first<br/>
        2. Event: {'{'}page: /products, timestamp: 10:30{'}'}<br/>
        3. Event: {'{'}page: /checkout, timestamp: 10:35{'}'}
      </div>
    </Box>

    <Box color={colors.orange} label="OUTPUT:" padding={12}>
      <div style={{fontSize: '12px', marginTop: '5px'}}>
        {'{'}page: /products, viewer_birth_year: 1990{'}'}<br/>
        {'{'}page: /checkout, viewer_birth_year: 1990{'}'}
      </div>
    </Box>
  </Column>
</DiagramContainer>

**How it works:**

1. Two mappers: one for events (emit `user_id ‚Üí event`), one for users (emit `user_id ‚Üí profile`)
2. Shuffle brings all records with same `user_id` to same reducer
3. **Secondary sort** ensures user profile arrives first
4. Reducer stores profile in variable, then iterates over events
5. Minimal memory: only one user's data in memory at a time

---

## 5. Query Languages and DataFrames

### 5.1. SQL for Batch Processing

**As batch systems matured, SQL became the lingua franca:**
- Familiar to analysts and engineers
- Optimizers can choose efficient execution plans
- Integrates with existing BI tools

```sql
-- Find top pages by age group
SELECT
    page,
    FLOOR((2025 - birth_year) / 10) * 10 AS age_decade,
    COUNT(*) AS views
FROM events e
JOIN users u ON e.user_id = u.user_id
GROUP BY page, age_decade
ORDER BY views DESC
LIMIT 100;
```

**Why SQL won:**

- Analysts and developers already know it
- Integrates with existing BI tools (Tableau, Looker)
- Query optimizers can choose efficient execution plans
- More concise than handwritten MapReduce

**SQL-based batch engines:**
- **Hive**: SQL on Hadoop/Spark
- **Trino (Presto)**: Federated SQL across data sources
- **Spark SQL**: SQL on Spark
- **BigQuery, Snowflake**: Cloud data warehouses

### 5.2. DataFrames

**Data scientists preferred the DataFrame model:**
- Familiar from R and Pandas
- Step-by-step transformations
- Works with SQL under the hood

```python
# Pandas-style DataFrame API (runs distributed on Spark)
events_df = spark.read.parquet("s3://data/events/")
users_df = spark.read.parquet("s3://data/users/")

result = (events_df
    .join(users_df, "user_id")
    .withColumn("age", 2025 - users_df.birth_year)
    .groupBy("page")
    .agg(
        count("*").alias("views"),
        avg("age").alias("avg_viewer_age")
    )
    .orderBy(desc("views"))
    .limit(100))

result.write.parquet("s3://output/page-demographics/")
```

| Aspect | SQL | DataFrame API |
|--------|-----|---------------|
| **Style** | Declarative (what) | Step-by-step (how) |
| **Optimization** | Query planner | Query planner (Spark) or immediate (Pandas) |
| **Familiarity** | DBAs, analysts | Data scientists |
| **Flexibility** | Standard operators | Custom functions easier |

> **üí° Insight**
>
> Spark's DataFrame API is deceptively clever:
>
> - Unlike Pandas which executes immediately, Spark builds a **query plan**
> - Optimizes the plan before execution
> - `df.filter(x).filter(y)` becomes a single optimized filter
> - Not two passes over the data

### 5.3. Batch Processing and Data Warehouses Converge

**Historically separate, batch processing and data warehouses are merging:**
- Same underlying technologies (columnar storage, distributed shuffle)
- Object storage (S3) as common foundation
- SQL + DataFrame APIs both supported

<DiagramContainer title="CONVERGENCE OF BATCH AND WAREHOUSES">
  <Column gap={30}>
    <div>
      <div style={{fontSize: '14px', fontWeight: 'bold', marginBottom: '15px', textAlign: 'center'}}>TRADITIONAL SEPARATION:</div>
      <Row gap={60}>
        <Column gap={10}>
          <div style={{fontSize: '13px', fontWeight: 'bold'}}>Batch Processing</div>
          <div style={{fontSize: '12px'}}>
            ‚Ä¢ MapReduce, Spark<br/>
            ‚Ä¢ Flexible code<br/>
            ‚Ä¢ Commodity hardware<br/>
            ‚Ä¢ Horizontal scaling
          </div>
        </Column>
        <Column gap={10}>
          <div style={{fontSize: '13px', fontWeight: 'bold'}}>Data Warehouse</div>
          <div style={{fontSize: '12px'}}>
            ‚Ä¢ Teradata, Oracle<br/>
            ‚Ä¢ SQL only<br/>
            ‚Ä¢ Specialized appliances<br/>
            ‚Ä¢ Vertical scaling
          </div>
        </Column>
      </Row>
    </div>

    <div style={{borderTop: '3px double #ccc', margin: '10px 0'}} />

    <div>
      <div style={{fontSize: '14px', fontWeight: 'bold', marginBottom: '15px', textAlign: 'center'}}>MODERN CONVERGENCE:</div>
      <Box color={colors.blue} label="Cloud Data Platforms (BigQuery, Snowflake, Databricks)" padding={15}>
        <div style={{fontSize: '12px', marginTop: '8px'}}>
          ‚Ä¢ SQL + DataFrame APIs<br/>
          ‚Ä¢ Columnar storage (Parquet)<br/>
          ‚Ä¢ Distributed shuffle<br/>
          ‚Ä¢ Object storage (S3) as foundation<br/>
          ‚Ä¢ Same engines for ETL and analytics
        </div>
      </Box>
    </div>
  </Column>
</DiagramContainer>

**When to use which:**

| Workload | Better fit |
|----------|------------|
| SQL analytics | Cloud warehouse (BigQuery, Snowflake) |
| Complex ML pipelines | Batch framework (Spark, Ray) |
| Row-by-row processing | Batch framework |
| Cost-sensitive large jobs | Batch framework |
| Iterative graph algorithms | Batch framework |

---

## 6. Batch Use Cases

### 6.1. Extract-Transform-Load (ETL)

**In plain English:**
- ETL is like a factory assembly line for data
- Raw materials (source data) come in
- Get processed and quality-checked (transformed)
- Packaged for shipping (loaded to destination)

<DiagramContainer title="ETL PIPELINE EXAMPLE">
  <Row gap={40} align="center">
    <Column gap={15}>
      <div style={{fontSize: '13px', fontWeight: 'bold', textAlign: 'center'}}>EXTRACT</div>
      <Column gap={12}>
        <Box color={colors.blue} label="Production Database (PostgreSQL)" padding={10} />
        <Box color={colors.blue} label="Application Logs (S3)" padding={10} />
        <Box color={colors.blue} label="Third-party API Exports" padding={10} />
      </Column>
    </Column>

    <Column gap={10} align="center">
      <Arrow direction="right" size={40} />
    </Column>

    <Column gap={15}>
      <div style={{fontSize: '13px', fontWeight: 'bold', textAlign: 'center'}}>TRANSFORM</div>
      <Box color={colors.green} padding={12}>
        <div style={{fontSize: '12px'}}>
          ‚Ä¢ Clean data<br/>
          ‚Ä¢ Join tables<br/>
          ‚Ä¢ Aggregate<br/>
          ‚Ä¢ Validate<br/>
          ‚Ä¢ Parse JSON<br/>
          ‚Ä¢ Filter<br/>
          ‚Ä¢ Enrich
        </div>
      </Box>
    </Column>

    <Column gap={10} align="center">
      <Arrow direction="right" size={40} />
    </Column>

    <Column gap={15}>
      <div style={{fontSize: '13px', fontWeight: 'bold', textAlign: 'center'}}>LOAD</div>
      <Column gap={12}>
        <Box color={colors.purple} label="Data Warehouse (Snowflake)" padding={10} />
        <Box color={colors.purple} label="ML Feature Store" padding={10} />
        <Box color={colors.purple} label="Search Index" padding={10} />
      </Column>
    </Column>
  </Row>
  <div style={{fontSize: '12px', textAlign: 'center', marginTop: '15px'}}>
    Workflow scheduler (Airflow) orchestrates the pipeline<br/>
    Runs daily/hourly, handles retries, alerts on failure
  </div>
</DiagramContainer>

**Why batch for ETL:**

- **Parallelizable**: Filtering, projecting, and joining are embarrassingly parallel
- **Debuggable**: Inspect failed files, fix code, rerun
- **Retryable**: Transient failures handled by scheduler
- **Orchestrated**: Airflow, Dagster provide operators for many systems

### 6.2. Analytics

**Batch systems support two analytics patterns:**
- Pre-aggregation (scheduled jobs)
- Ad hoc queries (interactive exploration)

**1. Pre-aggregation (scheduled)**
```sql
-- Runs daily via Airflow
CREATE TABLE daily_sales_cube AS
SELECT
    date,
    region,
    product_category,
    SUM(revenue) as total_revenue,
    COUNT(*) as transactions
FROM transactions
WHERE date = CURRENT_DATE - 1
GROUP BY date, region, product_category;
```

**2. Ad hoc queries (interactive)**
```sql
-- Analyst runs this to investigate spike
SELECT
    hour,
    COUNT(*) as errors,
    error_type
FROM logs
WHERE date = '2025-06-27' AND status >= 500
GROUP BY hour, error_type
ORDER BY errors DESC;
```

### 6.3. Machine Learning

**Batch processing is central to ML workflows:**
- Feature engineering at scale
- Model training (gradient descent across huge datasets)
- Batch inference (score millions of records)

<DiagramContainer title="ML BATCH PROCESSING WORKFLOW">
  <Column gap={25}>
    <div>
      <div style={{fontSize: '14px', fontWeight: 'bold', marginBottom: '8px'}}>1. FEATURE ENGINEERING</div>
      <ProcessFlow
        steps={[
          {label: "Raw Data (text, images)"},
          {label: "Clean (validated)"},
          {label: "Transform (normalized)"},
          {label: "Feature Vectors (numeric arrays)"}
        ]}
        color={colors.blue}
      />
    </div>

    <div>
      <div style={{fontSize: '14px', fontWeight: 'bold', marginBottom: '8px'}}>2. MODEL TRAINING</div>
      <ProcessFlow
        steps={[
          {label: "Training Data (features + labels)"},
          {label: "Batch Job (gradient descent)"},
          {label: "Model Weights (checkpoint files)"}
        ]}
        color={colors.green}
      />
    </div>

    <div>
      <div style={{fontSize: '14px', fontWeight: 'bold', marginBottom: '8px'}}>3. BATCH INFERENCE</div>
      <ProcessFlow
        steps={[
          {label: "New Data + Model (millions of rows)"},
          {label: "Batch Job (apply model)"},
          {label: "Predictions (recommendation scores)"}
        ]}
        color={colors.purple}
      />
    </div>

    <div style={{fontSize: '12px', textAlign: 'center', marginTop: '10px'}}>
      Frameworks: Spark MLlib, Ray, Kubeflow, Flyte<br/>
      OpenAI uses Ray for ChatGPT training
    </div>
  </Column>
</DiagramContainer>

**LLM data preparation** is a prime batch workload:
- Extract plain text from HTML
- Detect and remove duplicates
- Filter low-quality content
- Tokenize text into embeddings

### 6.4. Bulk Data Imports

**Batch outputs often need to reach production databases.**

**Why NOT write directly from batch jobs:**

| Problem | Why |
|---------|-----|
| **Slow** | Network request per record |
| **Overwhelming** | Thousands of tasks writing simultaneously |
| **Inconsistent** | Partial results visible if job fails |

**Better patterns:**

**1. Stream through Kafka**
<DiagramContainer>
  <Row gap={30} align="center">
    <Box color={colors.blue} label="Batch Output" padding={12} />
    <Arrow direction="right" />
    <Box color={colors.green} label="Kafka Topic" padding={12} />
    <Arrow direction="right" />
    <Box color={colors.purple} label="Elasticsearch (throttled)" padding={12} />
  </Row>
  <div style={{fontSize: '12px', textAlign: 'center', marginTop: '10px'}}>
    Buffer between batch and production<br/>
    Consumers control their read rate<br/>
    Multiple downstream systems can consume
  </div>
</DiagramContainer>

**2. Bulk file import**
<DiagramContainer>
  <Row gap={30} align="center">
    <Box color={colors.blue} label="Batch Job" padding={12} />
    <Arrow direction="right" />
    <Box color={colors.green} label="SST Files on S3" padding={12} />
    <Arrow direction="right" />
    <Box color={colors.purple} label="RocksDB (bulk load)" padding={12} />
  </Row>
  <div style={{fontSize: '12px', textAlign: 'center', marginTop: '10px'}}>
    Build database files in batch job<br/>
    Bulk load atomically<br/>
    Venice, Pinot, Druid support this pattern
  </div>
</DiagramContainer>

---

## 7. Summary

**In this chapter, we explored batch processing from Unix pipes to petabyte-scale systems.**

**Core concepts:**
- Batch processing transforms **immutable inputs** into **derived outputs**
- Same patterns from Unix (`sort | uniq -c`) scale to distributed systems
- Key operation is **shuffle**: redistribute data so same keys meet at same node

**System components:**
- **Distributed filesystems** (HDFS) and **object stores** (S3) provide storage
- **Orchestrators** (YARN, K8s) schedule tasks across machines
- **Workflow schedulers** (Airflow) manage job dependencies

**Processing models:**
- **MapReduce**: map ‚Üí shuffle ‚Üí reduce (largely obsolete)
- **Dataflow engines** (Spark, Flink): flexible operators, in-memory intermediate data
- **SQL** and **DataFrame APIs**: high-level, optimizable interfaces

**Use cases:**
- **ETL**: move and transform data between systems
- **Analytics**: pre-aggregation and ad hoc queries
- **Machine learning**: feature engineering, training, batch inference
- **Bulk imports**: populate production systems from batch outputs

> **üí° Insight**
>
> The key insight of batch processing is treating computation as **pure functions**:
>
> - Given the same input, produce the same output (no side effects)
> - Jobs become reproducible, retriable, and parallelizable
> - When something goes wrong, you can always rerun
> - That property is worth its weight in gold for building reliable systems

In Chapter 12, we'll turn to **stream processing**, where inputs are unbounded and jobs never complete. We'll see how stream processing builds on batch concepts while introducing new challenges around time, ordering, and state.

---

**Previous:** [Chapter 10. Consistency and Consensus](/ddia/part2/chapter10-consistency-consensus) | **Next:** [Chapter 12. Stream Processing](/ddia/part3/chapter12-stream-processing)
